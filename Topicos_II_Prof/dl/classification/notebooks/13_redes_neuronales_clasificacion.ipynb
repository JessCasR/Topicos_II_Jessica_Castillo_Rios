{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <span style=\"color:indigo\">Machine Learning e Inferencia Bayesiana</span> </center> \n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Centro_Universitario_del_Guadalajara_Logo.png/640px-Centro_Universitario_del_Guadalajara_Logo.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</center>\n",
    "    \n",
    "<center> <span style=\"color:DarkBlue\">  Tema 13: Redes neuronales, clasificacion </span>  </center>\n",
    "<center> <span style=\"color:Blue\"> M. en C. Iv√°n A. Toledano Ju√°rez </span>  </center>\n",
    "\n",
    "# Clasificaci√≥n con PyTorch\n",
    "\n",
    "Este notebook est√° basado en las notas de **MRDBourke** y utiliza datos del famoso dataset del **[Titanic](https://www.kaggle.com/competitions/titanic/overview)** disponible en Kaggle.  \n",
    "El objetivo es construir un modelo de **clasificaci√≥n binaria** con **PyTorch**, que prediga la probabilidad de supervivencia de los pasajeros.\n",
    "\n",
    "## Objetivo\n",
    "Entrenar una red neuronal simple que aprenda a clasificar a los pasajeros del Titanic seg√∫n las caracter√≠sticas disponibles, estimando si **sobrevivieron (1)** o **no sobrevivieron (0)**.\n",
    "\n",
    "## Variables del dataset\n",
    "\n",
    "| Variable | Descripci√≥n | Valores posibles |\n",
    "|-----------|--------------|------------------|\n",
    "| `survival` | Supervivencia | 0 = No, 1 = S√≠ |\n",
    "| `pclass` | Clase del boleto | 1 = 1¬™, 2 = 2¬™, 3 = 3¬™ |\n",
    "| `sex` | Sexo | ‚Äî |\n",
    "| `age` | Edad (en a√±os) | ‚Äî |\n",
    "| `sibsp` | N¬∫ de hermanos / c√≥nyuges a bordo | ‚Äî |\n",
    "| `parch` | N¬∫ de padres / hijos a bordo | ‚Äî |\n",
    "| `ticket` | N√∫mero de boleto | ‚Äî |\n",
    "| `fare` | Tarifa pagada | ‚Äî |\n",
    "| `cabin` | N√∫mero de cabina | ‚Äî |\n",
    "| `embarked` | Puerto de embarque | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "---\n",
    "\n",
    "A lo largo del notebook se realizar√° el **preprocesamiento de datos**, la **construcci√≥n del modelo**, y la **evaluaci√≥n de su desempe√±o** mediante m√©tricas de clasificaci√≥n como **exactitud** y **matriz de confusi√≥n**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked  Survived\n",
      "0       3    male  22.0      1      0   7.2500   NaN        S         0\n",
      "1       1  female  38.0      1      0  71.2833   C85        C         1\n",
      "2       3  female  26.0      0      0   7.9250   NaN        S         1\n",
      "3       1  female  35.0      1      0  53.1000  C123        S         1\n",
      "4       3    male  35.0      0      0   8.0500   NaN        S         0\n",
      "Shape(Train) (891, 9)\n",
      "Shape(test) (418, 8)\n"
     ]
    }
   ],
   "source": [
    "df_titanic_train = pd.read_csv(\"titanic_data/train.csv\")\n",
    "df_titanic_test = pd.read_csv(\"titanic_data/test.csv\")\n",
    "\n",
    "# Algunas variables no son necesarias\n",
    "\n",
    "variables = ['Pclass','Sex', 'Age', 'SibSp','Parch', 'Fare', 'Cabin', 'Embarked','Survived']\n",
    "variables_2 = variables.copy()\n",
    "variables_2.remove('Survived')\n",
    "\n",
    "df_titanic_train = df_titanic_train[variables]\n",
    "df_titanic_test = df_titanic_test[variables_2]\n",
    "\n",
    "\n",
    "print(df_titanic_train.head(5))\n",
    "\n",
    "print('Shape(Train)',df_titanic_train.shape)\n",
    "print('Shape(test)',df_titanic_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    891 non-null    int64  \n",
      " 1   Sex       891 non-null    object \n",
      " 2   Age       714 non-null    float64\n",
      " 3   SibSp     891 non-null    int64  \n",
      " 4   Parch     891 non-null    int64  \n",
      " 5   Fare      891 non-null    float64\n",
      " 6   Cabin     204 non-null    object \n",
      " 7   Embarked  889 non-null    object \n",
      " 8   Survived  891 non-null    int64  \n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 62.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_titanic_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "Pclass      0\n",
       "Sex         0\n",
       "Age       177\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        0\n",
       "Cabin     687\n",
       "Embarked    2\n",
       "Survived    0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores nulos\n",
    "df_titanic_train.isna().sum().to_frame().head(83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "Pclass      0\n",
       "Sex         0\n",
       "Age        86\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        1\n",
       "Cabin     327\n",
       "Embarked    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores nulos\n",
    "df_titanic_test.isna().sum().to_frame().head(83)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ignorar la variable Cabin. Para la variable de edad, (... podr√≠amos rellenarlo con la media .... o no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Pclass','Sex', 'Age', 'SibSp','Parch', 'Fare', 'Embarked','Survived']\n",
    "columns_2 = columns.copy()\n",
    "columns_2.remove('Survived')\n",
    "\n",
    "df2_titanic_train = df_titanic_train.copy()\n",
    "df2_titanic_test = df_titanic_test.copy()\n",
    "\n",
    "df2_titanic_train = df2_titanic_train[columns]\n",
    "df2_titanic_test = df2_titanic_test[columns_2]\n",
    "\n",
    "df2_titanic_train['Age'] = df2_titanic_train['Age'].fillna(df2_titanic_train['Age'].mean())\n",
    "df2_titanic_test['Age'] = df2_titanic_test['Age'].fillna(df2_titanic_test['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categoricas y num√©ricas\n",
    "\n",
    "categorical = df2_titanic_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical = df2_titanic_train.select_dtypes(include='number').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables dummy con pandas\n",
    "\n",
    "df3_titanic_train = df2_titanic_train.copy()\n",
    "df3_titanic_test = df2_titanic_test.copy()\n",
    "\n",
    "\n",
    "for element in categorical:\n",
    "    tab_dummy = pd.get_dummies(df3_titanic_train[element],prefix=element, dtype=int)\n",
    "    data_new = df3_titanic_train.join(tab_dummy)\n",
    "    df3_titanic_train = data_new\n",
    "    \n",
    "for element in categorical:\n",
    "    tab_dummy = pd.get_dummies(df3_titanic_test[element],prefix=element, dtype=int)\n",
    "    data_new = df3_titanic_test.join(tab_dummy)\n",
    "    df3_titanic_test = data_new\n",
    "\n",
    "# Quitamos las columnas redundantes\n",
    "to_keep = [element for element in df3_titanic_train.columns if element not in categorical]\n",
    "to_keep_2 = [element for element in df3_titanic_test.columns if element not in categorical]\n",
    "\n",
    "df3_titanic_train = df3_titanic_train[to_keep]\n",
    "df3_titanic_test = df3_titanic_test[to_keep_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass        Age  SibSp  Parch     Fare  Survived  Sex_female  Sex_male  \\\n",
       "0         3  22.000000      1      0   7.2500         0           0         1   \n",
       "1         1  38.000000      1      0  71.2833         1           1         0   \n",
       "2         3  26.000000      0      0   7.9250         1           1         0   \n",
       "3         1  35.000000      1      0  53.1000         1           1         0   \n",
       "4         3  35.000000      0      0   8.0500         0           0         1   \n",
       "..      ...        ...    ...    ...      ...       ...         ...       ...   \n",
       "886       2  27.000000      0      0  13.0000         0           0         1   \n",
       "887       1  19.000000      0      0  30.0000         1           1         0   \n",
       "888       3  29.699118      1      2  23.4500         0           1         0   \n",
       "889       1  26.000000      0      0  30.0000         1           0         1   \n",
       "890       3  32.000000      0      0   7.7500         0           0         1   \n",
       "\n",
       "     Embarked_C  Embarked_Q  Embarked_S  \n",
       "0             0           0           1  \n",
       "1             1           0           0  \n",
       "2             0           0           1  \n",
       "3             0           0           1  \n",
       "4             0           0           1  \n",
       "..          ...         ...         ...  \n",
       "886           0           0           1  \n",
       "887           0           0           1  \n",
       "888           0           0           1  \n",
       "889           1           0           0  \n",
       "890           0           1           0  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_titanic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features y class\n",
    "X_list = to_keep.copy()\n",
    "X_list.remove('Survived')\n",
    "Y_list = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes a array\n",
    "\n",
    "X_df = df3_titanic_train[X_list]\n",
    "Y_df = df3_titanic_train[Y_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_train, X_df_test, Y_df_train, Y_df_test = train_test_split(X_df,\n",
    "                                                                Y_df,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array_train = X_df_train.to_numpy()\n",
    "X_array_test = X_df_test.to_numpy()\n",
    "Y_array_train = Y_df_train.to_numpy()\n",
    "Y_array_test = Y_df_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , 45.5,  0. , ...,  0. ,  0. ,  1. ],\n",
       "       [ 2. , 23. ,  0. , ...,  0. ,  0. ,  1. ],\n",
       "       [ 3. , 32. ,  0. , ...,  0. ,  0. ,  1. ],\n",
       "       ...,\n",
       "       [ 3. , 41. ,  2. , ...,  0. ,  0. ,  1. ],\n",
       "       [ 1. , 14. ,  1. , ...,  0. ,  0. ,  1. ],\n",
       "       [ 1. , 21. ,  0. , ...,  0. ,  0. ,  1. ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de una red neuronal para clasificaci√≥n\n",
    "\n",
    "| Hiperpar√°metro | Clasificaci√≥n binaria | Clasificaci√≥n multiclase |\n",
    "| --- | --- | --- |\n",
    "| Capa de entrada | El mismo que el n√∫mero de variables de entrada | Igual que clasificaci√≥n binaria|\n",
    "| Capas ocultas | Depende del problema. Te√≥ricamente puede ir de 1 a infinito | Igual que clasificaci√≥n binaria|\n",
    "| Neuronas por capa oculta | Depende del probleme. Usualmente entre 10 y 512 | Igual que clasificaci√≥n binaria|\n",
    "| Capas de salida | 1 (una por clase) | Una por cada clase|\n",
    "| Funci√≥n de activaci√≥n de capas ocultas | T√≠pica: ReLU, pero puede ser cualquiera. | Igual que clasificaci√≥n binaria|\n",
    "| Funci√≥n de activaci√≥n de capa de salida | T√≠pica: Sigmoid | [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) |\n",
    "| Loss Function | [Binary crossentropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) | [Crossentropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)|\n",
    "| Optimizador | SGD, [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) | Igual que clasificaci√≥n binaria|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays a tensores, y sets de entrenamiento y validaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_array_train).type(torch.float)\n",
    "X_test = torch.from_numpy(X_array_test).type(torch.float)\n",
    "y_train = torch.from_numpy(Y_array_train).type(torch.float)\n",
    "y_test = torch.from_numpy(Y_array_test).type(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el modelo\n",
    "\n",
    "Ya tenemos nuestros datos listos, as√≠ que es momento de **construir un modelo** de clasificaci√≥n utilizando **PyTorch**.  \n",
    "El proceso lo dividiremos en varios pasos clave:\n",
    "\n",
    "1. Configurar c√≥digo **agn√≥stico al dispositivo** (CPU o GPU).  \n",
    "2. Construir un modelo **subclasificando `nn.Module`**.  \n",
    "3. Definir una **funci√≥n de p√©rdida** y un **optimizador**.  \n",
    "4. Crear un **bucle de entrenamiento**.\n",
    "\n",
    "La buena noticia es que ya hemos seguido estos pasos antes (en el notebook anterior), solo que ahora los **ajustaremos para un problema de clasificaci√≥n**.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraci√≥n del dispositivo\n",
    "\n",
    "Comenzamos importando las librer√≠as necesarias y preparando el entorno para que el modelo pueda ejecutarse en **CPU o GPU**, seg√∫n disponibilidad. Si tu equipo tiene acceso a una GPU compatible, pytorch la utilizar√° autom√°ticamente. Esto permite que todo --datos, modelos y tensores -- se gestionesn en el dispositivo adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fijar el tipo de hardware\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([712, 10])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shapes de tensores\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaci√≥n del modelo\n",
    "\n",
    "Queremos un modelo que reciba nuestros datos de entrada `X`(features) y produzca una predicci√≥n `y`, es decir, un tipo de problema supervisado. Para ello, definiremos una clase en python que,\n",
    "\n",
    "* Herede de `nn.module` (como todos los modelos de pytorch)\n",
    "* Cree dos capas lineales `nn.linear` en el constructor, con las dimensiones de entrada y salida adecuadas para nuestros datos.\n",
    "* Implemente un m√©todo `forward()` que defina la propagaci√≥n hacia adelante del modelo\n",
    "* Instanciamos el modelo y lo env√≠amos al dispositivo configurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelV0(\n",
       "  (layer_1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (layer_2): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Construimos la clase del modelo con la subclase nn.Module\n",
    "class ModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2. Creamos las capas de entrada (lineales) capaces de manejar los features de entrada y clase de salida\n",
    "        self.layer_1 = nn.Linear(in_features=10, out_features=20) # toma 10 features (X), produce 20 features\n",
    "        self.layer_2 = nn.Linear(in_features=20, out_features=1) # toma 20 features, produce 1 feature (y)\n",
    "    \n",
    "    # 3. Definimos un m√©todo para la propagaci√≥n (forward)\n",
    "    def forward(self, x):\n",
    "        # Regresa la capa de salida de layer_2, un solo features, con el mismo shape que y\n",
    "        # El calculo pasa sobre layer_1 y luego su output es el input de layer_2\n",
    "        return self.layer_2(self.layer_1(x)) \n",
    "\n",
    "# 4. Creamos una instancia con el modelo y se manda al hardware\n",
    "model_0 = ModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera capa (`layer_1`) recibe 2 caracter√≠sticas de entrada (`in_features=2`) y produce 5 salidas (`out_features=5`). Estas 5 salidas se conocen como unidades ocultas, y permiten al modelo aprender **patrones m√°s complejos**.\n",
    "\n",
    "La segunda capa (`layer_2`) toma esas 5 caracter√≠sticas y las transforma en una √∫nica salida (`out_features=1`), que corresponden a la predicci√≥n del modelo.\n",
    "\n",
    "**NOTA**: El n√∫mero de unidades ocultas las elige uno. M√°s unidades podr√≠an capturar patrones m√°s complejos, pero tambi√©n pueden provocar sobreajuste y entrenamiento m√°s lento.\n",
    "\n",
    "## `nn.Sequential`\n",
    "\n",
    "El m√©todo `nn.Sequential()` ejecuta la propagaci√≥n hacia adelante en el orden en que aparecen las capas, simplificando la sintaxis cuando no se requieren pasos intermedios personalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (1): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se replica el modelV0\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=10, out_features=20),\n",
    "    nn.Linear(in_features=20, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of predictions: 179, Shape: torch.Size([179, 1])\n",
      "Length of test samples: 179, Shape: torch.Size([179])\n",
      "\n",
      "First 10 predictions:\n",
      "tensor([[1.3630],\n",
      "        [1.5595],\n",
      "        [1.0445],\n",
      "        [0.4934],\n",
      "        [0.7088],\n",
      "        [1.5590],\n",
      "        [1.5053],\n",
      "        [0.7810],\n",
      "        [0.8900],\n",
      "        [1.0415]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "First 10 test labels:\n",
      "tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Hacemos predicciones con el modelo\n",
    "untrained_preds = model_0(X_test.to(device))\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\n",
    "print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de la funci√≥n de p√©rdida y el optimizador\n",
    "\n",
    "Ya hemos configurado modelos, as√≠ que ahora toca definir **c√≥mo aprender√°**, a trav√©s de una **funci√≥n de p√©rdida** (*loss function*) y un **optimizador**.\n",
    "\n",
    "En el notebook previo ya usamos estos conceptos, pero es importante notar que **diferentes tipos de problemas requieren distintas funciones de p√©rdida.**\n",
    "\n",
    "---\n",
    "\n",
    "## Funci√≥n de p√©rdida\n",
    "\n",
    "La funci√≥n de p√©rdida (tambi√©n llamada *cost function*) mide **qu√© tan equivocadas son las predicciones del modelo**. Mientras m√°s alto sea su valor, peor est√° aprendiendo el modelo.  El entrenamiento consiste en **minimizar esta p√©rdida**.\n",
    "\n",
    "Ejemplos comunes:\n",
    "\n",
    "| Funci√≥n / Optimizador | Tipo de problema | C√≥digo en PyTorch |\n",
    "|------------------------|------------------|-------------------|\n",
    "| Stochastic Gradient Descent (SGD) | Clasificaci√≥n, regresi√≥n, muchos otros | `torch.optim.SGD()` |\n",
    "| Adam Optimizer | Clasificaci√≥n, regresi√≥n, muchos otros | `torch.optim.Adam()` |\n",
    "| Binary Cross Entropy (BCE) | Clasificaci√≥n binaria | `torch.nn.BCELoss()` o `torch.nn.BCEWithLogitsLoss()` |\n",
    "| Cross Entropy | Clasificaci√≥n multiclase | `torch.nn.CrossEntropyLoss()` |\n",
    "| Mean Absolute Error (MAE) / L1 Loss | Regresi√≥n | `torch.nn.L1Loss()` |\n",
    "| Mean Squared Error (MSE) / L2 Loss | Regresi√≥n | `torch.nn.MSELoss()` |\n",
    "\n",
    "---\n",
    "\n",
    "## Elecci√≥n para nuestro caso\n",
    "\n",
    "Como estamos trabajando con un **problema de clasificaci√≥n binaria**, la opci√≥n m√°s adecuada es usar una **p√©rdida de entrop√≠a cruzada binaria** (*binary cross entropy loss*).\n",
    "\n",
    "PyTorch ofrece dos versiones:\n",
    "\n",
    "- `torch.nn.BCELoss()`  \n",
    "  Calcula la entrop√≠a cruzada binaria entre las predicciones y las etiquetas.\n",
    "  \n",
    "- `torch.nn.BCEWithLogitsLoss()`  \n",
    "  Hace lo mismo, pero **integra internamente una funci√≥n sigmoide** (`nn.Sigmoid`).  \n",
    "  Esto la hace **m√°s estable num√©ricamente** y generalmente se recomienda sobre la anterior.\n",
    "\n",
    "> üí° **Recomendaci√≥n:**  \n",
    "> Usa `torch.nn.BCEWithLogitsLoss()` en la mayor√≠a de los casos de clasificaci√≥n binaria.  \n",
    "> Evita aplicar manualmente un `Sigmoid` si utilizas esta versi√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## Optimizador\n",
    "\n",
    "El optimizador es el algoritmo que **ajusta los pesos del modelo** para minimizar la p√©rdida.  Podemos usar el cl√°sico **descenso de gradiente estoc√°stico (SGD)** o el m√°s moderno **Adam**. Ambos funcionan bien, pero empezaremos con **SGD** para mayor claridad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n",
    "\n",
    "# Optimizador\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), \n",
    "                            lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©trica de evaluaci√≥n (accuracy)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calcula si dos tensores son iguales\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "Antes de realizar el loop de entrenamiento, veamos que sale del modelo al realizar una propagaci√≥n hacia adelante, usando los datos de validacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3630],\n",
       "        [1.5595],\n",
       "        [1.0445],\n",
       "        [0.4934],\n",
       "        [0.7088]], device='mps:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los 5 primeros outputs\n",
    "y_logits = model_0(X_test.to(device))[:5]\n",
    "y_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el modelo todav√≠a **no ha sido entrenado**, sus salidas son esencialmente **valores aleatorios**.  Durante la **propagaci√≥n hacia adelante**, los datos pasan a trav√©s de las dos capas lineales definidas, las cuales aplican internamente la siguiente ecuaci√≥n:\n",
    "\n",
    "\\begin{equation}\n",
    "y = x \\cdot w^T + \\mathrm{bias}\n",
    "\\end{equation}\n",
    "\n",
    "Los valores resultantes $y$ de esta operaci√≥n, as√≠ como los que produce el modelo, se conocen como **_logits_**.  En un modelo puramente lineal, estos logits representar√≠an simplemente **valores num√©ricos** sin restricci√≥n en su rango (pueden ser negativos o positivos).\n",
    "\n",
    "Si aplicamos una **funci√≥n de activaci√≥n sigmoide** sobre ellos, podemos convertir dichos valores en **probabilidades** dentro del intervalo $(0, 1)$, lo que nos permite interpretar el resultado como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{probabilidad de clase positiva} = \\sigma(y) = \\frac{1}{1 + e^{-y}}\n",
    "\\end{equation}\n",
    "\n",
    "De este modo, al establecer un **umbral (threshold)** ‚Äîpor ejemplo, 0.5‚Äî podemos transformar las probabilidades en una **clasificaci√≥n binaria**:\n",
    "\n",
    "- Si $\\sigma(y) \\ge 0.5$ ‚Üí clase **1 (positivo)**  \n",
    "- Si $\\sigma(y) < 0.5$ ‚Üí clase **0 (negativo)**\n",
    "\n",
    "> üí° **Nota:** En PyTorch, cuando se utiliza `nn.BCEWithLogitsLoss`, la funci√≥n sigmoide ya est√° incorporada dentro de la funci√≥n de p√©rdida, por lo que **no es necesario aplicarla manualmente** en la salida del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7963],\n",
       "        [0.8263],\n",
       "        [0.7397],\n",
       "        [0.6209],\n",
       "        [0.6701]], device='mps:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "y_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='mps:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redondeamos para obtener una clasificaci√≥n (threshold 0.5)\n",
    "#y_preds = torch.round(y_pred_probs)\n",
    "threshold = 0.5\n",
    "y_preds = (y_pred_probs >= threshold).float()\n",
    "\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
    "\n",
    "# Checamos igualdad\n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
    "\n",
    "# Quitamos la dimensi√≥n extra\n",
    "y_preds.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1., 1.], device='mps:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]\n",
    "# Vemos que ahora si tenemos las etiquetas que queremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.16306, Accuracy: 37.78% | Test loss: 2.00418, Test acc: 58.66%\n",
      "Epoch: 10 | Loss: 1.53208, Accuracy: 43.40% | Test loss: 0.73070, Test acc: 65.36%\n",
      "Epoch: 20 | Loss: 0.77356, Accuracy: 64.47% | Test loss: 0.90248, Test acc: 58.66%\n",
      "Epoch: 30 | Loss: 0.64264, Accuracy: 68.12% | Test loss: 0.64960, Test acc: 62.57%\n",
      "Epoch: 40 | Loss: 1.02167, Accuracy: 61.52% | Test loss: 0.64553, Test acc: 64.80%\n",
      "Epoch: 50 | Loss: 0.71083, Accuracy: 66.85% | Test loss: 0.71363, Test acc: 58.66%\n",
      "Epoch: 60 | Loss: 0.68624, Accuracy: 67.98% | Test loss: 0.67134, Test acc: 59.78%\n",
      "Epoch: 70 | Loss: 0.69462, Accuracy: 68.12% | Test loss: 0.65710, Test acc: 59.22%\n",
      "Epoch: 80 | Loss: 0.67428, Accuracy: 67.98% | Test loss: 0.64648, Test acc: 59.22%\n",
      "Epoch: 90 | Loss: 0.65893, Accuracy: 67.84% | Test loss: 0.63801, Test acc: 59.78%\n",
      "Epoch: 100 | Loss: 0.64717, Accuracy: 68.12% | Test loss: 0.63110, Test acc: 60.89%\n",
      "Epoch: 110 | Loss: 0.63801, Accuracy: 68.54% | Test loss: 0.62537, Test acc: 61.45%\n",
      "Epoch: 120 | Loss: 0.63077, Accuracy: 69.24% | Test loss: 0.62054, Test acc: 62.01%\n",
      "Epoch: 130 | Loss: 0.62496, Accuracy: 70.22% | Test loss: 0.61643, Test acc: 62.01%\n",
      "Epoch: 140 | Loss: 0.62022, Accuracy: 70.65% | Test loss: 0.61290, Test acc: 62.01%\n",
      "Epoch: 150 | Loss: 0.61631, Accuracy: 71.21% | Test loss: 0.60983, Test acc: 62.57%\n",
      "Epoch: 160 | Loss: 0.61303, Accuracy: 71.21% | Test loss: 0.60714, Test acc: 63.13%\n",
      "Epoch: 170 | Loss: 0.61027, Accuracy: 71.49% | Test loss: 0.60477, Test acc: 63.13%\n",
      "Epoch: 180 | Loss: 0.60790, Accuracy: 71.49% | Test loss: 0.60265, Test acc: 63.13%\n",
      "Epoch: 190 | Loss: 0.60586, Accuracy: 71.63% | Test loss: 0.60074, Test acc: 63.69%\n",
      "Epoch: 200 | Loss: 0.60408, Accuracy: 71.63% | Test loss: 0.59901, Test acc: 63.69%\n",
      "Epoch: 210 | Loss: 0.60251, Accuracy: 71.63% | Test loss: 0.59743, Test acc: 64.25%\n",
      "Epoch: 220 | Loss: 0.60111, Accuracy: 71.77% | Test loss: 0.59596, Test acc: 64.25%\n",
      "Epoch: 230 | Loss: 0.59986, Accuracy: 71.77% | Test loss: 0.59460, Test acc: 64.25%\n",
      "Epoch: 240 | Loss: 0.59873, Accuracy: 71.91% | Test loss: 0.59332, Test acc: 64.25%\n",
      "Epoch: 250 | Loss: 0.59769, Accuracy: 71.63% | Test loss: 0.59211, Test acc: 64.25%\n",
      "Epoch: 260 | Loss: 0.59674, Accuracy: 71.63% | Test loss: 0.59096, Test acc: 64.25%\n",
      "Epoch: 270 | Loss: 0.59585, Accuracy: 72.05% | Test loss: 0.58986, Test acc: 64.80%\n",
      "Epoch: 280 | Loss: 0.59503, Accuracy: 71.77% | Test loss: 0.58879, Test acc: 64.80%\n",
      "Epoch: 290 | Loss: 0.59425, Accuracy: 71.63% | Test loss: 0.58777, Test acc: 64.80%\n",
      "Epoch: 300 | Loss: 0.59352, Accuracy: 71.77% | Test loss: 0.58677, Test acc: 64.80%\n",
      "Epoch: 310 | Loss: 0.59281, Accuracy: 71.63% | Test loss: 0.58580, Test acc: 65.92%\n",
      "Epoch: 320 | Loss: 0.59214, Accuracy: 71.63% | Test loss: 0.58486, Test acc: 65.92%\n",
      "Epoch: 330 | Loss: 0.59150, Accuracy: 71.63% | Test loss: 0.58393, Test acc: 65.92%\n",
      "Epoch: 340 | Loss: 0.59088, Accuracy: 71.63% | Test loss: 0.58302, Test acc: 65.92%\n",
      "Epoch: 350 | Loss: 0.59027, Accuracy: 71.63% | Test loss: 0.58213, Test acc: 65.92%\n",
      "Epoch: 360 | Loss: 0.58969, Accuracy: 71.77% | Test loss: 0.58125, Test acc: 65.92%\n",
      "Epoch: 370 | Loss: 0.58912, Accuracy: 71.63% | Test loss: 0.58039, Test acc: 65.92%\n",
      "Epoch: 380 | Loss: 0.58856, Accuracy: 71.63% | Test loss: 0.57954, Test acc: 65.92%\n",
      "Epoch: 390 | Loss: 0.58802, Accuracy: 71.91% | Test loss: 0.57870, Test acc: 65.92%\n",
      "Epoch: 400 | Loss: 0.58748, Accuracy: 71.77% | Test loss: 0.57787, Test acc: 65.92%\n",
      "Epoch: 410 | Loss: 0.58696, Accuracy: 71.91% | Test loss: 0.57705, Test acc: 67.04%\n",
      "Epoch: 420 | Loss: 0.58644, Accuracy: 72.05% | Test loss: 0.57624, Test acc: 67.04%\n",
      "Epoch: 430 | Loss: 0.58593, Accuracy: 72.05% | Test loss: 0.57545, Test acc: 67.60%\n",
      "Epoch: 440 | Loss: 0.58543, Accuracy: 72.33% | Test loss: 0.57466, Test acc: 67.60%\n",
      "Epoch: 450 | Loss: 0.58494, Accuracy: 72.61% | Test loss: 0.57388, Test acc: 68.16%\n",
      "Epoch: 460 | Loss: 0.58445, Accuracy: 72.75% | Test loss: 0.57310, Test acc: 68.16%\n",
      "Epoch: 470 | Loss: 0.58397, Accuracy: 72.61% | Test loss: 0.57234, Test acc: 68.16%\n",
      "Epoch: 480 | Loss: 0.58350, Accuracy: 72.61% | Test loss: 0.57159, Test acc: 68.16%\n",
      "Epoch: 490 | Loss: 0.58303, Accuracy: 72.61% | Test loss: 0.57084, Test acc: 68.16%\n",
      "Epoch: 500 | Loss: 0.58256, Accuracy: 72.75% | Test loss: 0.57010, Test acc: 68.16%\n",
      "Epoch: 510 | Loss: 0.58210, Accuracy: 72.89% | Test loss: 0.56937, Test acc: 67.60%\n",
      "Epoch: 520 | Loss: 0.58165, Accuracy: 73.46% | Test loss: 0.56865, Test acc: 67.60%\n",
      "Epoch: 530 | Loss: 0.58120, Accuracy: 73.60% | Test loss: 0.56793, Test acc: 67.60%\n",
      "Epoch: 540 | Loss: 0.58075, Accuracy: 73.60% | Test loss: 0.56723, Test acc: 67.60%\n",
      "Epoch: 550 | Loss: 0.58031, Accuracy: 73.88% | Test loss: 0.56653, Test acc: 67.60%\n",
      "Epoch: 560 | Loss: 0.57987, Accuracy: 73.88% | Test loss: 0.56583, Test acc: 67.60%\n",
      "Epoch: 570 | Loss: 0.57943, Accuracy: 73.88% | Test loss: 0.56515, Test acc: 67.60%\n",
      "Epoch: 580 | Loss: 0.57900, Accuracy: 74.16% | Test loss: 0.56447, Test acc: 67.60%\n",
      "Epoch: 590 | Loss: 0.57857, Accuracy: 74.58% | Test loss: 0.56380, Test acc: 67.60%\n",
      "Epoch: 600 | Loss: 0.57814, Accuracy: 74.72% | Test loss: 0.56313, Test acc: 67.60%\n",
      "Epoch: 610 | Loss: 0.57772, Accuracy: 74.72% | Test loss: 0.56247, Test acc: 67.60%\n",
      "Epoch: 620 | Loss: 0.57730, Accuracy: 75.00% | Test loss: 0.56182, Test acc: 67.60%\n",
      "Epoch: 630 | Loss: 0.57688, Accuracy: 74.86% | Test loss: 0.56118, Test acc: 67.60%\n",
      "Epoch: 640 | Loss: 0.57647, Accuracy: 74.86% | Test loss: 0.56054, Test acc: 67.60%\n",
      "Epoch: 650 | Loss: 0.57605, Accuracy: 74.72% | Test loss: 0.55991, Test acc: 67.60%\n",
      "Epoch: 660 | Loss: 0.57564, Accuracy: 74.86% | Test loss: 0.55928, Test acc: 67.60%\n",
      "Epoch: 670 | Loss: 0.57523, Accuracy: 74.86% | Test loss: 0.55866, Test acc: 67.60%\n",
      "Epoch: 680 | Loss: 0.57483, Accuracy: 74.86% | Test loss: 0.55805, Test acc: 67.60%\n",
      "Epoch: 690 | Loss: 0.57442, Accuracy: 75.14% | Test loss: 0.55744, Test acc: 67.60%\n",
      "Epoch: 700 | Loss: 0.57402, Accuracy: 75.28% | Test loss: 0.55684, Test acc: 67.60%\n",
      "Epoch: 710 | Loss: 0.57362, Accuracy: 75.28% | Test loss: 0.55624, Test acc: 68.16%\n",
      "Epoch: 720 | Loss: 0.57323, Accuracy: 75.42% | Test loss: 0.55565, Test acc: 68.16%\n",
      "Epoch: 730 | Loss: 0.57283, Accuracy: 75.42% | Test loss: 0.55507, Test acc: 68.16%\n",
      "Epoch: 740 | Loss: 0.57244, Accuracy: 75.56% | Test loss: 0.55449, Test acc: 68.16%\n",
      "Epoch: 750 | Loss: 0.57204, Accuracy: 75.56% | Test loss: 0.55392, Test acc: 68.16%\n",
      "Epoch: 760 | Loss: 0.57165, Accuracy: 75.56% | Test loss: 0.55335, Test acc: 68.16%\n",
      "Epoch: 770 | Loss: 0.57127, Accuracy: 75.84% | Test loss: 0.55279, Test acc: 68.16%\n",
      "Epoch: 780 | Loss: 0.57088, Accuracy: 75.84% | Test loss: 0.55223, Test acc: 68.16%\n",
      "Epoch: 790 | Loss: 0.57050, Accuracy: 75.84% | Test loss: 0.55168, Test acc: 68.16%\n",
      "Epoch: 800 | Loss: 0.57011, Accuracy: 75.98% | Test loss: 0.55113, Test acc: 68.16%\n",
      "Epoch: 810 | Loss: 0.56973, Accuracy: 76.12% | Test loss: 0.55059, Test acc: 68.16%\n",
      "Epoch: 820 | Loss: 0.56935, Accuracy: 75.84% | Test loss: 0.55005, Test acc: 68.16%\n",
      "Epoch: 830 | Loss: 0.56897, Accuracy: 75.70% | Test loss: 0.54952, Test acc: 68.72%\n",
      "Epoch: 840 | Loss: 0.56860, Accuracy: 75.42% | Test loss: 0.54899, Test acc: 68.72%\n",
      "Epoch: 850 | Loss: 0.56822, Accuracy: 75.42% | Test loss: 0.54847, Test acc: 68.72%\n",
      "Epoch: 860 | Loss: 0.56785, Accuracy: 75.56% | Test loss: 0.54795, Test acc: 68.72%\n",
      "Epoch: 870 | Loss: 0.56748, Accuracy: 75.42% | Test loss: 0.54743, Test acc: 68.72%\n",
      "Epoch: 880 | Loss: 0.56711, Accuracy: 75.56% | Test loss: 0.54692, Test acc: 68.72%\n",
      "Epoch: 890 | Loss: 0.56674, Accuracy: 75.56% | Test loss: 0.54642, Test acc: 68.72%\n",
      "Epoch: 900 | Loss: 0.56638, Accuracy: 75.56% | Test loss: 0.54592, Test acc: 68.72%\n",
      "Epoch: 910 | Loss: 0.56601, Accuracy: 75.56% | Test loss: 0.54542, Test acc: 68.72%\n",
      "Epoch: 920 | Loss: 0.56565, Accuracy: 75.56% | Test loss: 0.54493, Test acc: 68.72%\n",
      "Epoch: 930 | Loss: 0.56529, Accuracy: 75.84% | Test loss: 0.54444, Test acc: 68.72%\n",
      "Epoch: 940 | Loss: 0.56493, Accuracy: 75.84% | Test loss: 0.54396, Test acc: 68.72%\n",
      "Epoch: 950 | Loss: 0.56457, Accuracy: 75.84% | Test loss: 0.54348, Test acc: 68.72%\n",
      "Epoch: 960 | Loss: 0.56421, Accuracy: 75.98% | Test loss: 0.54300, Test acc: 69.27%\n",
      "Epoch: 970 | Loss: 0.56386, Accuracy: 75.98% | Test loss: 0.54253, Test acc: 69.27%\n",
      "Epoch: 980 | Loss: 0.56350, Accuracy: 76.40% | Test loss: 0.54206, Test acc: 69.27%\n",
      "Epoch: 990 | Loss: 0.56315, Accuracy: 77.11% | Test loss: 0.54159, Test acc: 69.27%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88) # semilla aleatoria\n",
    "\n",
    "# N√∫mero de epocas\n",
    "epochs = 1000\n",
    "\n",
    "# Poner los datos en el hardware target\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "# Loop de training y eval\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward propagation (el modelo regresa logits)\n",
    "    y_logits = model_0(X_train).squeeze() # squeeze para remover `1` dimension extra\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Se calcula loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss acepta los logits de salida\n",
    "                   y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Zero grad para el optimizador\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizador\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Evaluacion\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward \n",
    "        test_logits = model_0(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        \n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print cada 10 epocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "        train_loss_list.append(loss)\n",
    "        test_loss_list.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.163063  , 1.5320771 , 0.7735554 , 0.6426399 , 1.0216701 ,\n",
       "       0.7108254 , 0.6862402 , 0.69462234, 0.67428136, 0.65892565,\n",
       "       0.6471669 , 0.638014  , 0.6307739 , 0.624958  , 0.62021875,\n",
       "       0.6163052 , 0.6130331 , 0.61026603, 0.60790056, 0.6058575 ,\n",
       "       0.60407543, 0.6025059 , 0.6011107 , 0.5998588 , 0.59872586,\n",
       "       0.5976911 , 0.5967386 , 0.5958549 , 0.59502894, 0.59425163,\n",
       "       0.5935157 , 0.5928149 , 0.59214425, 0.5914993 , 0.59087706,\n",
       "       0.59027416, 0.58968836, 0.58911777, 0.5885606 , 0.5880154 ,\n",
       "       0.5874809 , 0.58695614, 0.5864405 , 0.58593297, 0.585433  ,\n",
       "       0.58493984, 0.5844534 , 0.58397293, 0.58349806, 0.58302855,\n",
       "       0.58256406, 0.58210427, 0.58164907, 0.58119804, 0.5807511 ,\n",
       "       0.580308  , 0.57986844, 0.57943255, 0.5789999 , 0.5785703 ,\n",
       "       0.5781439 , 0.5777204 , 0.5772996 , 0.5768816 , 0.57646614,\n",
       "       0.5760531 , 0.57564247, 0.5752344 , 0.57482845, 0.57442456,\n",
       "       0.5740229 , 0.5736235 , 0.5732258 , 0.5728303 , 0.57243645,\n",
       "       0.5720448 , 0.57165474, 0.5712667 , 0.5708804 , 0.5704959 ,\n",
       "       0.5701131 , 0.56973207, 0.5693526 , 0.568975  , 0.5685989 ,\n",
       "       0.56822455, 0.5678519 , 0.56748056, 0.5671111 , 0.56674325,\n",
       "       0.5663769 , 0.5660121 , 0.56564885, 0.5652873 , 0.5649272 ,\n",
       "       0.56456876, 0.5642117 , 0.5638562 , 0.5635024 , 0.5631497 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list_cpu = torch.Tensor(train_loss_list).cpu().detach().numpy()\n",
    "train_loss_list_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0041788 , 0.7307008 , 0.9024806 , 0.6495987 , 0.64553213,\n",
       "       0.7136336 , 0.67134345, 0.6571004 , 0.6464777 , 0.63801265,\n",
       "       0.63110256, 0.62536746, 0.6205414 , 0.6164325 , 0.6128982 ,\n",
       "       0.6098295 , 0.60714114, 0.6047655 , 0.6026471 , 0.60074157,\n",
       "       0.5990119 , 0.5974278 , 0.5959644 , 0.5946011 , 0.59332126,\n",
       "       0.59211075, 0.59095854, 0.5898555 , 0.5887939 , 0.58776766,\n",
       "       0.5867718 , 0.58580226, 0.58485574, 0.58392936, 0.58302116,\n",
       "       0.5821292 , 0.5812519 , 0.58038825, 0.57953703, 0.5786977 ,\n",
       "       0.57786924, 0.5770513 , 0.5762434 , 0.57544506, 0.574656  ,\n",
       "       0.57387596, 0.57310456, 0.5723418 , 0.57158726, 0.57084095,\n",
       "       0.5701026 , 0.5693722 , 0.5686494 , 0.56793433, 0.56722665,\n",
       "       0.56652635, 0.56583333, 0.5651475 , 0.56446886, 0.563797  ,\n",
       "       0.56313205, 0.56247395, 0.5618224 , 0.56117743, 0.5605391 ,\n",
       "       0.5599071 , 0.55928123, 0.55866176, 0.55804837, 0.5574408 ,\n",
       "       0.5568393 , 0.5562436 , 0.5556537 , 0.5550694 , 0.5544906 ,\n",
       "       0.5539174 , 0.55334955, 0.5527869 , 0.55222964, 0.5516774 ,\n",
       "       0.55113024, 0.5505881 , 0.5500506 , 0.5495182 , 0.5489904 ,\n",
       "       0.5484672 , 0.54794866, 0.5474345 , 0.54692495, 0.5464196 ,\n",
       "       0.5459185 , 0.5454217 , 0.54492897, 0.5444404 , 0.5439558 ,\n",
       "       0.54347503, 0.5429983 , 0.54252523, 0.542056  , 0.5415904 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_list_cpu = torch.Tensor(test_loss_list).cpu().detach().numpy()\n",
    "test_loss_list_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_array = np.arange(0,100,1)\n",
    "plt.plot(x_array, train_loss_list_cpu, color='blue', label=\"train loss\")\n",
    "plt.plot(x_array, test_loss_list_cpu, color='orange', label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejorando el modelo\n",
    "\n",
    "Una vez que el modelo b√°sico est√° funcionando, existen diversas estrategias para **mejorar su desempe√±o**.  Cada una de las siguientes t√©cnicas busca aumentar la capacidad del modelo para **aprender patrones m√°s complejos** o **ajustarse mejor a los datos**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. A√±adir m√°s capas\n",
    "Cada capa adicional puede incrementar la **capacidad de representaci√≥n** del modelo, permiti√©ndole aprender **patrones m√°s abstractos y no lineales**. Agregar m√°s capas hace que la red sea m√°s **profunda**, lo que da origen al t√©rmino *deep learning*.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. A√±adir m√°s neuronas ocultas\n",
    "De forma similar, aumentar el n√∫mero de **neuronas (unidades ocultas)** dentro de una capa puede mejorar la capacidad del modelo para capturar relaciones complejas entre las variables. Sin embargo, demasiadas neuronas pueden llevar al **sobreajuste (overfitting)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Entrenar por m√°s √©pocas\n",
    "Dar al modelo m√°s **√©pocas** (iteraciones completas sobre los datos) permite que los pesos se actualicen m√°s veces, lo que puede mejorar el rendimiento si el modelo a√∫n no ha convergido. Pero un n√∫mero excesivo de √©pocas tambi√©n puede causar **sobreajuste**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Cambiar la funci√≥n de activaci√≥n\n",
    "Los datos reales rara vez son lineales. Usar funciones de activaci√≥n **no lineales** (como ReLU, tanh o sigmoid) permite que el modelo aprenda relaciones m√°s complejas.  \n",
    "Por ejemplo:\n",
    "- `nn.ReLU()` ‚Üí com√∫n en redes profundas.  \n",
    "- `nn.Sigmoid()` ‚Üí √∫til en clasificaci√≥n binaria.  \n",
    "- `nn.Tanh()` ‚Üí centrada en 0, √∫til para ciertos tipos de datos.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ajustar la tasa de aprendizaje\n",
    "La **tasa de aprendizaje** (`learning_rate`) controla qu√© tanto se ajustan los par√°metros en cada actualizaci√≥n.  \n",
    "- Si es **demasiado alta**, el modelo puede **oscilar o divergir**.  \n",
    "- Si es **demasiado baja**, el aprendizaje ser√° **muy lento** o se quedar√° estancado en un m√≠nimo local.  \n",
    "\n",
    "Encontrar un valor adecuado requiere **experimentaci√≥n o b√∫squeda sistem√°tica (grid/random search)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Cambiar la funci√≥n de p√©rdida\n",
    "Cada tipo de problema (clasificaci√≥n binaria, multiclase, regresi√≥n, etc.) requiere una **funci√≥n de p√©rdida diferente**.  \n",
    "Probar distintas opciones puede mejorar la estabilidad o precisi√≥n del aprendizaje.\n",
    "\n",
    "Ejemplo:\n",
    "- Clasificaci√≥n binaria ‚Üí `nn.BCEWithLogitsLoss()`\n",
    "- Clasificaci√≥n multiclase ‚Üí `nn.CrossEntropyLoss()`\n",
    "- Regresi√≥n ‚Üí `nn.MSELoss()` o `nn.L1Loss()`\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Transfer learning\n",
    "En lugar de entrenar un modelo desde cero, se puede **aprovechar un modelo preentrenado** en un problema similar y **ajustarlo (fine-tuning)** a los nuevos datos.  \n",
    "Esta t√©cnica es muy √∫til cuando se dispone de **pocos datos** o se trabaja con **dominios complejos**, como im√°genes o texto.\n",
    "\n",
    "---\n",
    "\n",
    "> **NOTA:** No existe una receta √∫nica para mejorar el modelo.  \n",
    "> La pr√°ctica m√°s com√∫n es **ajustar un hiperpar√°metro a la vez**, observar su impacto en la p√©rdida y en las m√©tricas de validaci√≥n, y repetir el proceso hasta encontrar un equilibrio entre **precisi√≥n y generalizaci√≥n**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelV1(\n",
       "  (layer_1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (layer_2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (layer_3): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=10, out_features=20)\n",
    "        self.layer_2 = nn.Linear(in_features=20, out_features=20) # capa extra\n",
    "        self.layer_3 = nn.Linear(in_features=20, out_features=1)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        # z = self.layer_1(x)\n",
    "        # z = self.layer_2(z)\n",
    "        # z = self.layer_3(z)\n",
    "        # return z\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
    "\n",
    "model_1 = ModelV1().to(device)\n",
    "model_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.48872, Accuracy: 80.06% | Test loss: 0.47162, Test acc: 77.09%\n",
      "Epoch: 10 | Loss: 0.48860, Accuracy: 80.06% | Test loss: 0.47149, Test acc: 77.09%\n",
      "Epoch: 20 | Loss: 0.48848, Accuracy: 80.06% | Test loss: 0.47137, Test acc: 77.09%\n",
      "Epoch: 30 | Loss: 0.48836, Accuracy: 80.06% | Test loss: 0.47124, Test acc: 76.54%\n",
      "Epoch: 40 | Loss: 0.48825, Accuracy: 80.06% | Test loss: 0.47111, Test acc: 76.54%\n",
      "Epoch: 50 | Loss: 0.48813, Accuracy: 80.06% | Test loss: 0.47098, Test acc: 76.54%\n",
      "Epoch: 60 | Loss: 0.48801, Accuracy: 80.06% | Test loss: 0.47086, Test acc: 76.54%\n",
      "Epoch: 70 | Loss: 0.48789, Accuracy: 80.06% | Test loss: 0.47073, Test acc: 76.54%\n",
      "Epoch: 80 | Loss: 0.48778, Accuracy: 80.06% | Test loss: 0.47061, Test acc: 76.54%\n",
      "Epoch: 90 | Loss: 0.48766, Accuracy: 80.06% | Test loss: 0.47048, Test acc: 77.09%\n",
      "Epoch: 100 | Loss: 0.48755, Accuracy: 80.06% | Test loss: 0.47036, Test acc: 77.09%\n",
      "Epoch: 110 | Loss: 0.48743, Accuracy: 80.20% | Test loss: 0.47023, Test acc: 77.09%\n",
      "Epoch: 120 | Loss: 0.48732, Accuracy: 80.20% | Test loss: 0.47011, Test acc: 77.09%\n",
      "Epoch: 130 | Loss: 0.48720, Accuracy: 80.20% | Test loss: 0.46999, Test acc: 77.09%\n",
      "Epoch: 140 | Loss: 0.48709, Accuracy: 80.20% | Test loss: 0.46987, Test acc: 77.09%\n",
      "Epoch: 150 | Loss: 0.48698, Accuracy: 80.20% | Test loss: 0.46974, Test acc: 77.09%\n",
      "Epoch: 160 | Loss: 0.48687, Accuracy: 79.92% | Test loss: 0.46962, Test acc: 77.09%\n",
      "Epoch: 170 | Loss: 0.48676, Accuracy: 79.92% | Test loss: 0.46950, Test acc: 77.09%\n",
      "Epoch: 180 | Loss: 0.48664, Accuracy: 79.78% | Test loss: 0.46938, Test acc: 77.09%\n",
      "Epoch: 190 | Loss: 0.48653, Accuracy: 79.78% | Test loss: 0.46926, Test acc: 77.09%\n",
      "Epoch: 200 | Loss: 0.48642, Accuracy: 79.78% | Test loss: 0.46914, Test acc: 77.09%\n",
      "Epoch: 210 | Loss: 0.48631, Accuracy: 79.92% | Test loss: 0.46902, Test acc: 77.09%\n",
      "Epoch: 220 | Loss: 0.48621, Accuracy: 79.92% | Test loss: 0.46891, Test acc: 77.09%\n",
      "Epoch: 230 | Loss: 0.48610, Accuracy: 79.92% | Test loss: 0.46879, Test acc: 77.09%\n",
      "Epoch: 240 | Loss: 0.48599, Accuracy: 79.92% | Test loss: 0.46867, Test acc: 77.09%\n",
      "Epoch: 250 | Loss: 0.48588, Accuracy: 79.92% | Test loss: 0.46855, Test acc: 77.09%\n",
      "Epoch: 260 | Loss: 0.48578, Accuracy: 79.78% | Test loss: 0.46844, Test acc: 77.09%\n",
      "Epoch: 270 | Loss: 0.48567, Accuracy: 79.78% | Test loss: 0.46832, Test acc: 77.09%\n",
      "Epoch: 280 | Loss: 0.48556, Accuracy: 79.78% | Test loss: 0.46820, Test acc: 77.09%\n",
      "Epoch: 290 | Loss: 0.48546, Accuracy: 79.78% | Test loss: 0.46809, Test acc: 77.09%\n",
      "Epoch: 300 | Loss: 0.48535, Accuracy: 79.78% | Test loss: 0.46797, Test acc: 77.09%\n",
      "Epoch: 310 | Loss: 0.48525, Accuracy: 79.92% | Test loss: 0.46786, Test acc: 77.09%\n",
      "Epoch: 320 | Loss: 0.48514, Accuracy: 79.92% | Test loss: 0.46775, Test acc: 77.09%\n",
      "Epoch: 330 | Loss: 0.48504, Accuracy: 79.92% | Test loss: 0.46763, Test acc: 77.09%\n",
      "Epoch: 340 | Loss: 0.48494, Accuracy: 79.92% | Test loss: 0.46752, Test acc: 77.09%\n",
      "Epoch: 350 | Loss: 0.48484, Accuracy: 79.92% | Test loss: 0.46741, Test acc: 77.09%\n",
      "Epoch: 360 | Loss: 0.48473, Accuracy: 80.06% | Test loss: 0.46730, Test acc: 77.09%\n",
      "Epoch: 370 | Loss: 0.48463, Accuracy: 80.06% | Test loss: 0.46718, Test acc: 77.09%\n",
      "Epoch: 380 | Loss: 0.48453, Accuracy: 80.06% | Test loss: 0.46707, Test acc: 77.09%\n",
      "Epoch: 390 | Loss: 0.48443, Accuracy: 80.34% | Test loss: 0.46696, Test acc: 77.09%\n",
      "Epoch: 400 | Loss: 0.48433, Accuracy: 80.34% | Test loss: 0.46685, Test acc: 77.09%\n",
      "Epoch: 410 | Loss: 0.48423, Accuracy: 80.62% | Test loss: 0.46674, Test acc: 77.09%\n",
      "Epoch: 420 | Loss: 0.48413, Accuracy: 80.62% | Test loss: 0.46663, Test acc: 77.09%\n",
      "Epoch: 430 | Loss: 0.48403, Accuracy: 80.62% | Test loss: 0.46652, Test acc: 77.09%\n",
      "Epoch: 440 | Loss: 0.48393, Accuracy: 80.62% | Test loss: 0.46641, Test acc: 77.09%\n",
      "Epoch: 450 | Loss: 0.48384, Accuracy: 80.62% | Test loss: 0.46631, Test acc: 77.09%\n",
      "Epoch: 460 | Loss: 0.48374, Accuracy: 80.62% | Test loss: 0.46620, Test acc: 77.09%\n",
      "Epoch: 470 | Loss: 0.48364, Accuracy: 80.62% | Test loss: 0.46609, Test acc: 77.09%\n",
      "Epoch: 480 | Loss: 0.48354, Accuracy: 80.62% | Test loss: 0.46598, Test acc: 77.09%\n",
      "Epoch: 490 | Loss: 0.48345, Accuracy: 80.62% | Test loss: 0.46588, Test acc: 77.09%\n",
      "Epoch: 500 | Loss: 0.48335, Accuracy: 80.62% | Test loss: 0.46577, Test acc: 77.09%\n",
      "Epoch: 510 | Loss: 0.48326, Accuracy: 80.62% | Test loss: 0.46566, Test acc: 77.09%\n",
      "Epoch: 520 | Loss: 0.48316, Accuracy: 80.62% | Test loss: 0.46556, Test acc: 77.09%\n",
      "Epoch: 530 | Loss: 0.48307, Accuracy: 80.62% | Test loss: 0.46545, Test acc: 77.09%\n",
      "Epoch: 540 | Loss: 0.48297, Accuracy: 80.62% | Test loss: 0.46535, Test acc: 77.09%\n",
      "Epoch: 550 | Loss: 0.48288, Accuracy: 80.62% | Test loss: 0.46525, Test acc: 77.09%\n",
      "Epoch: 560 | Loss: 0.48279, Accuracy: 80.62% | Test loss: 0.46514, Test acc: 77.09%\n",
      "Epoch: 570 | Loss: 0.48269, Accuracy: 80.76% | Test loss: 0.46504, Test acc: 77.09%\n",
      "Epoch: 580 | Loss: 0.48260, Accuracy: 80.76% | Test loss: 0.46494, Test acc: 77.09%\n",
      "Epoch: 590 | Loss: 0.48251, Accuracy: 80.76% | Test loss: 0.46483, Test acc: 77.09%\n",
      "Epoch: 600 | Loss: 0.48242, Accuracy: 80.76% | Test loss: 0.46473, Test acc: 77.09%\n",
      "Epoch: 610 | Loss: 0.48233, Accuracy: 80.76% | Test loss: 0.46463, Test acc: 77.09%\n",
      "Epoch: 620 | Loss: 0.48223, Accuracy: 80.76% | Test loss: 0.46453, Test acc: 77.09%\n",
      "Epoch: 630 | Loss: 0.48214, Accuracy: 80.76% | Test loss: 0.46443, Test acc: 77.09%\n",
      "Epoch: 640 | Loss: 0.48205, Accuracy: 80.76% | Test loss: 0.46433, Test acc: 77.09%\n",
      "Epoch: 650 | Loss: 0.48196, Accuracy: 80.76% | Test loss: 0.46423, Test acc: 77.09%\n",
      "Epoch: 660 | Loss: 0.48188, Accuracy: 80.62% | Test loss: 0.46413, Test acc: 77.65%\n",
      "Epoch: 670 | Loss: 0.48179, Accuracy: 80.62% | Test loss: 0.46403, Test acc: 77.65%\n",
      "Epoch: 680 | Loss: 0.48170, Accuracy: 80.62% | Test loss: 0.46393, Test acc: 77.65%\n",
      "Epoch: 690 | Loss: 0.48161, Accuracy: 80.62% | Test loss: 0.46383, Test acc: 77.65%\n",
      "Epoch: 700 | Loss: 0.48152, Accuracy: 80.62% | Test loss: 0.46373, Test acc: 77.65%\n",
      "Epoch: 710 | Loss: 0.48144, Accuracy: 80.62% | Test loss: 0.46363, Test acc: 77.65%\n",
      "Epoch: 720 | Loss: 0.48135, Accuracy: 80.62% | Test loss: 0.46353, Test acc: 77.65%\n",
      "Epoch: 730 | Loss: 0.48126, Accuracy: 80.62% | Test loss: 0.46344, Test acc: 77.65%\n",
      "Epoch: 740 | Loss: 0.48118, Accuracy: 80.62% | Test loss: 0.46334, Test acc: 77.65%\n",
      "Epoch: 750 | Loss: 0.48109, Accuracy: 80.62% | Test loss: 0.46324, Test acc: 77.65%\n",
      "Epoch: 760 | Loss: 0.48100, Accuracy: 80.62% | Test loss: 0.46315, Test acc: 77.65%\n",
      "Epoch: 770 | Loss: 0.48092, Accuracy: 80.62% | Test loss: 0.46305, Test acc: 77.65%\n",
      "Epoch: 780 | Loss: 0.48083, Accuracy: 80.62% | Test loss: 0.46296, Test acc: 77.65%\n",
      "Epoch: 790 | Loss: 0.48075, Accuracy: 80.62% | Test loss: 0.46286, Test acc: 77.65%\n",
      "Epoch: 800 | Loss: 0.48067, Accuracy: 80.62% | Test loss: 0.46277, Test acc: 77.65%\n",
      "Epoch: 810 | Loss: 0.48058, Accuracy: 80.76% | Test loss: 0.46267, Test acc: 77.65%\n",
      "Epoch: 820 | Loss: 0.48050, Accuracy: 80.62% | Test loss: 0.46258, Test acc: 77.65%\n",
      "Epoch: 830 | Loss: 0.48042, Accuracy: 80.62% | Test loss: 0.46249, Test acc: 77.65%\n",
      "Epoch: 840 | Loss: 0.48033, Accuracy: 81.04% | Test loss: 0.46239, Test acc: 77.65%\n",
      "Epoch: 850 | Loss: 0.48025, Accuracy: 81.04% | Test loss: 0.46230, Test acc: 77.65%\n",
      "Epoch: 860 | Loss: 0.48017, Accuracy: 81.04% | Test loss: 0.46221, Test acc: 77.65%\n",
      "Epoch: 870 | Loss: 0.48009, Accuracy: 81.04% | Test loss: 0.46211, Test acc: 77.65%\n",
      "Epoch: 880 | Loss: 0.48001, Accuracy: 81.04% | Test loss: 0.46202, Test acc: 77.65%\n",
      "Epoch: 890 | Loss: 0.47993, Accuracy: 81.18% | Test loss: 0.46193, Test acc: 77.65%\n",
      "Epoch: 900 | Loss: 0.47985, Accuracy: 81.18% | Test loss: 0.46184, Test acc: 77.65%\n",
      "Epoch: 910 | Loss: 0.47977, Accuracy: 81.18% | Test loss: 0.46175, Test acc: 77.65%\n",
      "Epoch: 920 | Loss: 0.47969, Accuracy: 81.18% | Test loss: 0.46166, Test acc: 77.65%\n",
      "Epoch: 930 | Loss: 0.47961, Accuracy: 81.18% | Test loss: 0.46157, Test acc: 77.65%\n",
      "Epoch: 940 | Loss: 0.47953, Accuracy: 81.18% | Test loss: 0.46148, Test acc: 77.65%\n",
      "Epoch: 950 | Loss: 0.47945, Accuracy: 81.18% | Test loss: 0.46139, Test acc: 77.65%\n",
      "Epoch: 960 | Loss: 0.47937, Accuracy: 81.18% | Test loss: 0.46130, Test acc: 77.65%\n",
      "Epoch: 970 | Loss: 0.47929, Accuracy: 81.18% | Test loss: 0.46121, Test acc: 77.65%\n",
      "Epoch: 980 | Loss: 0.47922, Accuracy: 81.18% | Test loss: 0.46112, Test acc: 77.65%\n",
      "Epoch: 990 | Loss: 0.47914, Accuracy: 81.18% | Test loss: 0.46104, Test acc: 77.65%\n",
      "Epoch: 1000 | Loss: 0.47906, Accuracy: 81.18% | Test loss: 0.46095, Test acc: 77.65%\n",
      "Epoch: 1010 | Loss: 0.47898, Accuracy: 81.18% | Test loss: 0.46086, Test acc: 77.65%\n",
      "Epoch: 1020 | Loss: 0.47891, Accuracy: 81.18% | Test loss: 0.46077, Test acc: 77.65%\n",
      "Epoch: 1030 | Loss: 0.47883, Accuracy: 81.18% | Test loss: 0.46069, Test acc: 77.65%\n",
      "Epoch: 1040 | Loss: 0.47876, Accuracy: 81.18% | Test loss: 0.46060, Test acc: 77.65%\n",
      "Epoch: 1050 | Loss: 0.47868, Accuracy: 81.18% | Test loss: 0.46051, Test acc: 77.65%\n",
      "Epoch: 1060 | Loss: 0.47861, Accuracy: 81.18% | Test loss: 0.46043, Test acc: 77.65%\n",
      "Epoch: 1070 | Loss: 0.47853, Accuracy: 81.18% | Test loss: 0.46034, Test acc: 77.65%\n",
      "Epoch: 1080 | Loss: 0.47846, Accuracy: 81.18% | Test loss: 0.46026, Test acc: 77.65%\n",
      "Epoch: 1090 | Loss: 0.47838, Accuracy: 81.18% | Test loss: 0.46017, Test acc: 77.65%\n",
      "Epoch: 1100 | Loss: 0.47831, Accuracy: 81.18% | Test loss: 0.46009, Test acc: 77.65%\n",
      "Epoch: 1110 | Loss: 0.47823, Accuracy: 81.32% | Test loss: 0.46000, Test acc: 77.65%\n",
      "Epoch: 1120 | Loss: 0.47816, Accuracy: 81.32% | Test loss: 0.45992, Test acc: 77.65%\n",
      "Epoch: 1130 | Loss: 0.47809, Accuracy: 81.32% | Test loss: 0.45984, Test acc: 77.65%\n",
      "Epoch: 1140 | Loss: 0.47802, Accuracy: 81.32% | Test loss: 0.45975, Test acc: 77.65%\n",
      "Epoch: 1150 | Loss: 0.47794, Accuracy: 81.32% | Test loss: 0.45967, Test acc: 77.65%\n",
      "Epoch: 1160 | Loss: 0.47787, Accuracy: 81.32% | Test loss: 0.45959, Test acc: 78.21%\n",
      "Epoch: 1170 | Loss: 0.47780, Accuracy: 81.32% | Test loss: 0.45950, Test acc: 78.21%\n",
      "Epoch: 1180 | Loss: 0.47773, Accuracy: 81.32% | Test loss: 0.45942, Test acc: 78.21%\n",
      "Epoch: 1190 | Loss: 0.47766, Accuracy: 81.32% | Test loss: 0.45934, Test acc: 78.21%\n",
      "Epoch: 1200 | Loss: 0.47759, Accuracy: 81.18% | Test loss: 0.45926, Test acc: 78.21%\n",
      "Epoch: 1210 | Loss: 0.47752, Accuracy: 81.32% | Test loss: 0.45918, Test acc: 78.21%\n",
      "Epoch: 1220 | Loss: 0.47745, Accuracy: 81.32% | Test loss: 0.45910, Test acc: 78.21%\n",
      "Epoch: 1230 | Loss: 0.47738, Accuracy: 81.32% | Test loss: 0.45902, Test acc: 78.21%\n",
      "Epoch: 1240 | Loss: 0.47731, Accuracy: 81.32% | Test loss: 0.45894, Test acc: 78.21%\n",
      "Epoch: 1250 | Loss: 0.47724, Accuracy: 81.32% | Test loss: 0.45886, Test acc: 78.21%\n",
      "Epoch: 1260 | Loss: 0.47717, Accuracy: 81.32% | Test loss: 0.45878, Test acc: 78.21%\n",
      "Epoch: 1270 | Loss: 0.47710, Accuracy: 81.32% | Test loss: 0.45870, Test acc: 78.21%\n",
      "Epoch: 1280 | Loss: 0.47703, Accuracy: 81.32% | Test loss: 0.45862, Test acc: 78.21%\n",
      "Epoch: 1290 | Loss: 0.47696, Accuracy: 81.32% | Test loss: 0.45854, Test acc: 78.21%\n",
      "Epoch: 1300 | Loss: 0.47689, Accuracy: 81.32% | Test loss: 0.45846, Test acc: 78.21%\n",
      "Epoch: 1310 | Loss: 0.47683, Accuracy: 81.32% | Test loss: 0.45838, Test acc: 78.21%\n",
      "Epoch: 1320 | Loss: 0.47676, Accuracy: 81.32% | Test loss: 0.45830, Test acc: 78.21%\n",
      "Epoch: 1330 | Loss: 0.47669, Accuracy: 81.32% | Test loss: 0.45822, Test acc: 78.21%\n",
      "Epoch: 1340 | Loss: 0.47662, Accuracy: 81.32% | Test loss: 0.45815, Test acc: 78.21%\n",
      "Epoch: 1350 | Loss: 0.47656, Accuracy: 81.32% | Test loss: 0.45807, Test acc: 78.21%\n",
      "Epoch: 1360 | Loss: 0.47649, Accuracy: 81.32% | Test loss: 0.45799, Test acc: 78.21%\n",
      "Epoch: 1370 | Loss: 0.47642, Accuracy: 81.32% | Test loss: 0.45792, Test acc: 78.21%\n",
      "Epoch: 1380 | Loss: 0.47636, Accuracy: 81.32% | Test loss: 0.45784, Test acc: 78.21%\n",
      "Epoch: 1390 | Loss: 0.47629, Accuracy: 81.32% | Test loss: 0.45776, Test acc: 78.21%\n",
      "Epoch: 1400 | Loss: 0.47623, Accuracy: 81.32% | Test loss: 0.45769, Test acc: 78.21%\n",
      "Epoch: 1410 | Loss: 0.47616, Accuracy: 81.32% | Test loss: 0.45761, Test acc: 78.21%\n",
      "Epoch: 1420 | Loss: 0.47610, Accuracy: 81.32% | Test loss: 0.45754, Test acc: 78.21%\n",
      "Epoch: 1430 | Loss: 0.47603, Accuracy: 81.32% | Test loss: 0.45746, Test acc: 78.21%\n",
      "Epoch: 1440 | Loss: 0.47597, Accuracy: 81.32% | Test loss: 0.45739, Test acc: 78.21%\n",
      "Epoch: 1450 | Loss: 0.47590, Accuracy: 81.32% | Test loss: 0.45731, Test acc: 78.21%\n",
      "Epoch: 1460 | Loss: 0.47584, Accuracy: 81.32% | Test loss: 0.45724, Test acc: 78.21%\n",
      "Epoch: 1470 | Loss: 0.47578, Accuracy: 81.32% | Test loss: 0.45716, Test acc: 78.21%\n",
      "Epoch: 1480 | Loss: 0.47571, Accuracy: 81.32% | Test loss: 0.45709, Test acc: 78.21%\n",
      "Epoch: 1490 | Loss: 0.47565, Accuracy: 81.32% | Test loss: 0.45702, Test acc: 78.21%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88) # semilla aleatoria\n",
    "\n",
    "# N√∫mero de epocas\n",
    "epochs = 1500\n",
    "\n",
    "# Poner los datos en el hardware target\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop de training y eval\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward propagation (el modelo regresa logits)\n",
    "    y_logits = model_1(X_train).squeeze() # squeeze para remover `1` dimension extra\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Se calcula loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss acepta los logits de salida\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Zero grad para el optimizador\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizador\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Evaluacion\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward \n",
    "        test_logits = model_1(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print cada 10 epocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelV2(\n",
      "  (layer_1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (layer_2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (layer_3): Linear(in_features=20, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=10, out_features=20)\n",
    "        self.layer_2 = nn.Linear(in_features=20, out_features=20)\n",
    "        self.layer_3 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.relu = nn.ReLU() # <- Se a√±ade funci√≥n de activaci√≥n ReLU\n",
    "        # Tambi√©n se puede usar sigmoid, pero se tendr√≠a que quitar en la parte de transformaci√≥n del output \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "      # ReLU se aplica entre capas\n",
    "       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
    "\n",
    "model_3 = ModelV2().to(device)\n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.47559, Accuracy: 81.32% | Test loss: 0.45694, Test acc: 78.21%\n",
      "Epoch: 10 | Loss: 0.47553, Accuracy: 81.32% | Test loss: 0.45687, Test acc: 78.21%\n",
      "Epoch: 20 | Loss: 0.47546, Accuracy: 81.32% | Test loss: 0.45680, Test acc: 78.21%\n",
      "Epoch: 30 | Loss: 0.47540, Accuracy: 81.32% | Test loss: 0.45673, Test acc: 78.21%\n",
      "Epoch: 40 | Loss: 0.47534, Accuracy: 81.32% | Test loss: 0.45665, Test acc: 78.21%\n",
      "Epoch: 50 | Loss: 0.47528, Accuracy: 81.32% | Test loss: 0.45658, Test acc: 78.21%\n",
      "Epoch: 60 | Loss: 0.47522, Accuracy: 81.32% | Test loss: 0.45651, Test acc: 78.21%\n",
      "Epoch: 70 | Loss: 0.47516, Accuracy: 81.32% | Test loss: 0.45644, Test acc: 78.21%\n",
      "Epoch: 80 | Loss: 0.47509, Accuracy: 81.32% | Test loss: 0.45637, Test acc: 78.21%\n",
      "Epoch: 90 | Loss: 0.47503, Accuracy: 81.32% | Test loss: 0.45630, Test acc: 78.21%\n",
      "Epoch: 100 | Loss: 0.47497, Accuracy: 81.32% | Test loss: 0.45623, Test acc: 78.21%\n",
      "Epoch: 110 | Loss: 0.47491, Accuracy: 81.32% | Test loss: 0.45616, Test acc: 78.21%\n",
      "Epoch: 120 | Loss: 0.47485, Accuracy: 81.18% | Test loss: 0.45609, Test acc: 77.65%\n",
      "Epoch: 130 | Loss: 0.47479, Accuracy: 81.18% | Test loss: 0.45602, Test acc: 77.65%\n",
      "Epoch: 140 | Loss: 0.47473, Accuracy: 81.18% | Test loss: 0.45595, Test acc: 77.65%\n",
      "Epoch: 150 | Loss: 0.47468, Accuracy: 81.04% | Test loss: 0.45588, Test acc: 77.65%\n",
      "Epoch: 160 | Loss: 0.47462, Accuracy: 81.04% | Test loss: 0.45581, Test acc: 77.65%\n",
      "Epoch: 170 | Loss: 0.47456, Accuracy: 81.04% | Test loss: 0.45574, Test acc: 77.65%\n",
      "Epoch: 180 | Loss: 0.47450, Accuracy: 81.04% | Test loss: 0.45567, Test acc: 77.65%\n",
      "Epoch: 190 | Loss: 0.47444, Accuracy: 81.04% | Test loss: 0.45560, Test acc: 77.65%\n",
      "Epoch: 200 | Loss: 0.47438, Accuracy: 81.04% | Test loss: 0.45553, Test acc: 77.65%\n",
      "Epoch: 210 | Loss: 0.47432, Accuracy: 81.04% | Test loss: 0.45547, Test acc: 77.65%\n",
      "Epoch: 220 | Loss: 0.47427, Accuracy: 81.04% | Test loss: 0.45540, Test acc: 77.65%\n",
      "Epoch: 230 | Loss: 0.47421, Accuracy: 81.04% | Test loss: 0.45533, Test acc: 77.65%\n",
      "Epoch: 240 | Loss: 0.47415, Accuracy: 81.04% | Test loss: 0.45526, Test acc: 77.65%\n",
      "Epoch: 250 | Loss: 0.47410, Accuracy: 81.04% | Test loss: 0.45520, Test acc: 77.65%\n",
      "Epoch: 260 | Loss: 0.47404, Accuracy: 81.04% | Test loss: 0.45513, Test acc: 77.65%\n",
      "Epoch: 270 | Loss: 0.47398, Accuracy: 81.04% | Test loss: 0.45506, Test acc: 77.65%\n",
      "Epoch: 280 | Loss: 0.47393, Accuracy: 81.04% | Test loss: 0.45500, Test acc: 77.65%\n",
      "Epoch: 290 | Loss: 0.47387, Accuracy: 81.04% | Test loss: 0.45493, Test acc: 77.65%\n",
      "Epoch: 300 | Loss: 0.47381, Accuracy: 81.04% | Test loss: 0.45486, Test acc: 77.65%\n",
      "Epoch: 310 | Loss: 0.47376, Accuracy: 81.04% | Test loss: 0.45480, Test acc: 77.65%\n",
      "Epoch: 320 | Loss: 0.47370, Accuracy: 81.04% | Test loss: 0.45473, Test acc: 77.65%\n",
      "Epoch: 330 | Loss: 0.47365, Accuracy: 81.04% | Test loss: 0.45467, Test acc: 77.65%\n",
      "Epoch: 340 | Loss: 0.47359, Accuracy: 81.04% | Test loss: 0.45460, Test acc: 77.65%\n",
      "Epoch: 350 | Loss: 0.47354, Accuracy: 81.04% | Test loss: 0.45454, Test acc: 77.65%\n",
      "Epoch: 360 | Loss: 0.47348, Accuracy: 81.04% | Test loss: 0.45447, Test acc: 77.65%\n",
      "Epoch: 370 | Loss: 0.47343, Accuracy: 81.04% | Test loss: 0.45441, Test acc: 77.65%\n",
      "Epoch: 380 | Loss: 0.47337, Accuracy: 81.04% | Test loss: 0.45434, Test acc: 77.65%\n",
      "Epoch: 390 | Loss: 0.47332, Accuracy: 81.04% | Test loss: 0.45428, Test acc: 77.65%\n",
      "Epoch: 400 | Loss: 0.47327, Accuracy: 81.04% | Test loss: 0.45422, Test acc: 77.65%\n",
      "Epoch: 410 | Loss: 0.47321, Accuracy: 81.04% | Test loss: 0.45415, Test acc: 77.65%\n",
      "Epoch: 420 | Loss: 0.47316, Accuracy: 81.18% | Test loss: 0.45409, Test acc: 77.65%\n",
      "Epoch: 430 | Loss: 0.47311, Accuracy: 81.18% | Test loss: 0.45403, Test acc: 77.65%\n",
      "Epoch: 440 | Loss: 0.47305, Accuracy: 81.18% | Test loss: 0.45396, Test acc: 77.65%\n",
      "Epoch: 450 | Loss: 0.47300, Accuracy: 81.18% | Test loss: 0.45390, Test acc: 77.65%\n",
      "Epoch: 460 | Loss: 0.47295, Accuracy: 81.18% | Test loss: 0.45384, Test acc: 77.65%\n",
      "Epoch: 470 | Loss: 0.47289, Accuracy: 81.18% | Test loss: 0.45377, Test acc: 77.65%\n",
      "Epoch: 480 | Loss: 0.47284, Accuracy: 81.18% | Test loss: 0.45371, Test acc: 77.65%\n",
      "Epoch: 490 | Loss: 0.47279, Accuracy: 81.18% | Test loss: 0.45365, Test acc: 77.65%\n",
      "Epoch: 500 | Loss: 0.47274, Accuracy: 81.18% | Test loss: 0.45359, Test acc: 77.65%\n",
      "Epoch: 510 | Loss: 0.47269, Accuracy: 81.18% | Test loss: 0.45353, Test acc: 77.65%\n",
      "Epoch: 520 | Loss: 0.47264, Accuracy: 81.18% | Test loss: 0.45347, Test acc: 77.65%\n",
      "Epoch: 530 | Loss: 0.47258, Accuracy: 81.18% | Test loss: 0.45341, Test acc: 77.65%\n",
      "Epoch: 540 | Loss: 0.47253, Accuracy: 81.18% | Test loss: 0.45334, Test acc: 77.65%\n",
      "Epoch: 550 | Loss: 0.47248, Accuracy: 81.18% | Test loss: 0.45328, Test acc: 77.65%\n",
      "Epoch: 560 | Loss: 0.47243, Accuracy: 81.18% | Test loss: 0.45322, Test acc: 77.65%\n",
      "Epoch: 570 | Loss: 0.47238, Accuracy: 81.18% | Test loss: 0.45316, Test acc: 77.65%\n",
      "Epoch: 580 | Loss: 0.47233, Accuracy: 81.18% | Test loss: 0.45310, Test acc: 77.65%\n",
      "Epoch: 590 | Loss: 0.47228, Accuracy: 81.18% | Test loss: 0.45304, Test acc: 77.65%\n",
      "Epoch: 600 | Loss: 0.47223, Accuracy: 81.18% | Test loss: 0.45298, Test acc: 77.65%\n",
      "Epoch: 610 | Loss: 0.47218, Accuracy: 81.18% | Test loss: 0.45292, Test acc: 77.65%\n",
      "Epoch: 620 | Loss: 0.47213, Accuracy: 81.18% | Test loss: 0.45286, Test acc: 77.65%\n",
      "Epoch: 630 | Loss: 0.47208, Accuracy: 81.18% | Test loss: 0.45281, Test acc: 77.65%\n",
      "Epoch: 640 | Loss: 0.47203, Accuracy: 81.18% | Test loss: 0.45275, Test acc: 77.65%\n",
      "Epoch: 650 | Loss: 0.47198, Accuracy: 81.18% | Test loss: 0.45269, Test acc: 77.65%\n",
      "Epoch: 660 | Loss: 0.47193, Accuracy: 81.18% | Test loss: 0.45263, Test acc: 77.65%\n",
      "Epoch: 670 | Loss: 0.47189, Accuracy: 81.18% | Test loss: 0.45257, Test acc: 77.65%\n",
      "Epoch: 680 | Loss: 0.47184, Accuracy: 81.18% | Test loss: 0.45251, Test acc: 77.65%\n",
      "Epoch: 690 | Loss: 0.47179, Accuracy: 81.18% | Test loss: 0.45245, Test acc: 77.65%\n",
      "Epoch: 700 | Loss: 0.47174, Accuracy: 81.18% | Test loss: 0.45240, Test acc: 77.65%\n",
      "Epoch: 710 | Loss: 0.47169, Accuracy: 81.18% | Test loss: 0.45234, Test acc: 77.65%\n",
      "Epoch: 720 | Loss: 0.47164, Accuracy: 81.18% | Test loss: 0.45228, Test acc: 77.65%\n",
      "Epoch: 730 | Loss: 0.47160, Accuracy: 81.18% | Test loss: 0.45222, Test acc: 77.65%\n",
      "Epoch: 740 | Loss: 0.47155, Accuracy: 81.18% | Test loss: 0.45217, Test acc: 77.65%\n",
      "Epoch: 750 | Loss: 0.47150, Accuracy: 81.18% | Test loss: 0.45211, Test acc: 77.65%\n",
      "Epoch: 760 | Loss: 0.47145, Accuracy: 81.18% | Test loss: 0.45205, Test acc: 77.65%\n",
      "Epoch: 770 | Loss: 0.47141, Accuracy: 81.18% | Test loss: 0.45200, Test acc: 77.65%\n",
      "Epoch: 780 | Loss: 0.47136, Accuracy: 81.18% | Test loss: 0.45194, Test acc: 77.65%\n",
      "Epoch: 790 | Loss: 0.47131, Accuracy: 81.18% | Test loss: 0.45188, Test acc: 77.65%\n",
      "Epoch: 800 | Loss: 0.47127, Accuracy: 81.18% | Test loss: 0.45183, Test acc: 77.65%\n",
      "Epoch: 810 | Loss: 0.47122, Accuracy: 81.18% | Test loss: 0.45177, Test acc: 77.65%\n",
      "Epoch: 820 | Loss: 0.47117, Accuracy: 81.18% | Test loss: 0.45172, Test acc: 77.65%\n",
      "Epoch: 830 | Loss: 0.47113, Accuracy: 81.18% | Test loss: 0.45166, Test acc: 77.65%\n",
      "Epoch: 840 | Loss: 0.47108, Accuracy: 81.18% | Test loss: 0.45161, Test acc: 77.65%\n",
      "Epoch: 850 | Loss: 0.47104, Accuracy: 81.18% | Test loss: 0.45155, Test acc: 77.65%\n",
      "Epoch: 860 | Loss: 0.47099, Accuracy: 81.18% | Test loss: 0.45150, Test acc: 77.65%\n",
      "Epoch: 870 | Loss: 0.47095, Accuracy: 81.18% | Test loss: 0.45144, Test acc: 77.65%\n",
      "Epoch: 880 | Loss: 0.47090, Accuracy: 81.18% | Test loss: 0.45139, Test acc: 77.65%\n",
      "Epoch: 890 | Loss: 0.47086, Accuracy: 81.18% | Test loss: 0.45133, Test acc: 77.65%\n",
      "Epoch: 900 | Loss: 0.47081, Accuracy: 81.18% | Test loss: 0.45128, Test acc: 77.65%\n",
      "Epoch: 910 | Loss: 0.47077, Accuracy: 81.18% | Test loss: 0.45122, Test acc: 77.65%\n",
      "Epoch: 920 | Loss: 0.47072, Accuracy: 81.18% | Test loss: 0.45117, Test acc: 77.65%\n",
      "Epoch: 930 | Loss: 0.47068, Accuracy: 81.18% | Test loss: 0.45112, Test acc: 77.65%\n",
      "Epoch: 940 | Loss: 0.47063, Accuracy: 81.18% | Test loss: 0.45106, Test acc: 77.65%\n",
      "Epoch: 950 | Loss: 0.47059, Accuracy: 81.18% | Test loss: 0.45101, Test acc: 77.65%\n",
      "Epoch: 960 | Loss: 0.47054, Accuracy: 81.18% | Test loss: 0.45096, Test acc: 77.65%\n",
      "Epoch: 970 | Loss: 0.47050, Accuracy: 81.18% | Test loss: 0.45090, Test acc: 77.65%\n",
      "Epoch: 980 | Loss: 0.47046, Accuracy: 81.18% | Test loss: 0.45085, Test acc: 77.65%\n",
      "Epoch: 990 | Loss: 0.47041, Accuracy: 81.18% | Test loss: 0.45080, Test acc: 78.21%\n",
      "Epoch: 1000 | Loss: 0.47037, Accuracy: 81.18% | Test loss: 0.45074, Test acc: 78.21%\n",
      "Epoch: 1010 | Loss: 0.47033, Accuracy: 81.18% | Test loss: 0.45069, Test acc: 78.21%\n",
      "Epoch: 1020 | Loss: 0.47028, Accuracy: 81.18% | Test loss: 0.45064, Test acc: 78.21%\n",
      "Epoch: 1030 | Loss: 0.47024, Accuracy: 81.18% | Test loss: 0.45059, Test acc: 78.21%\n",
      "Epoch: 1040 | Loss: 0.47020, Accuracy: 81.18% | Test loss: 0.45053, Test acc: 78.21%\n",
      "Epoch: 1050 | Loss: 0.47016, Accuracy: 81.18% | Test loss: 0.45048, Test acc: 78.21%\n",
      "Epoch: 1060 | Loss: 0.47011, Accuracy: 81.32% | Test loss: 0.45043, Test acc: 78.21%\n",
      "Epoch: 1070 | Loss: 0.47007, Accuracy: 81.32% | Test loss: 0.45038, Test acc: 78.21%\n",
      "Epoch: 1080 | Loss: 0.47003, Accuracy: 81.32% | Test loss: 0.45033, Test acc: 78.21%\n",
      "Epoch: 1090 | Loss: 0.46999, Accuracy: 81.18% | Test loss: 0.45028, Test acc: 78.21%\n",
      "Epoch: 1100 | Loss: 0.46994, Accuracy: 81.18% | Test loss: 0.45023, Test acc: 78.21%\n",
      "Epoch: 1110 | Loss: 0.46990, Accuracy: 81.18% | Test loss: 0.45017, Test acc: 78.21%\n",
      "Epoch: 1120 | Loss: 0.46986, Accuracy: 81.18% | Test loss: 0.45012, Test acc: 78.21%\n",
      "Epoch: 1130 | Loss: 0.46982, Accuracy: 81.32% | Test loss: 0.45007, Test acc: 78.21%\n",
      "Epoch: 1140 | Loss: 0.46978, Accuracy: 81.32% | Test loss: 0.45002, Test acc: 78.21%\n",
      "Epoch: 1150 | Loss: 0.46974, Accuracy: 81.32% | Test loss: 0.44997, Test acc: 78.21%\n",
      "Epoch: 1160 | Loss: 0.46970, Accuracy: 81.60% | Test loss: 0.44992, Test acc: 78.21%\n",
      "Epoch: 1170 | Loss: 0.46965, Accuracy: 81.60% | Test loss: 0.44987, Test acc: 78.21%\n",
      "Epoch: 1180 | Loss: 0.46961, Accuracy: 81.60% | Test loss: 0.44982, Test acc: 78.21%\n",
      "Epoch: 1190 | Loss: 0.46957, Accuracy: 81.60% | Test loss: 0.44977, Test acc: 78.21%\n",
      "Epoch: 1200 | Loss: 0.46953, Accuracy: 81.60% | Test loss: 0.44972, Test acc: 78.21%\n",
      "Epoch: 1210 | Loss: 0.46949, Accuracy: 81.60% | Test loss: 0.44967, Test acc: 78.21%\n",
      "Epoch: 1220 | Loss: 0.46945, Accuracy: 81.60% | Test loss: 0.44962, Test acc: 78.21%\n",
      "Epoch: 1230 | Loss: 0.46941, Accuracy: 81.60% | Test loss: 0.44957, Test acc: 78.21%\n",
      "Epoch: 1240 | Loss: 0.46937, Accuracy: 81.60% | Test loss: 0.44953, Test acc: 78.21%\n",
      "Epoch: 1250 | Loss: 0.46933, Accuracy: 81.60% | Test loss: 0.44948, Test acc: 78.21%\n",
      "Epoch: 1260 | Loss: 0.46929, Accuracy: 81.60% | Test loss: 0.44943, Test acc: 78.21%\n",
      "Epoch: 1270 | Loss: 0.46925, Accuracy: 81.60% | Test loss: 0.44938, Test acc: 78.21%\n",
      "Epoch: 1280 | Loss: 0.46921, Accuracy: 81.60% | Test loss: 0.44933, Test acc: 78.21%\n",
      "Epoch: 1290 | Loss: 0.46917, Accuracy: 81.60% | Test loss: 0.44928, Test acc: 78.21%\n",
      "Epoch: 1300 | Loss: 0.46913, Accuracy: 81.60% | Test loss: 0.44923, Test acc: 78.21%\n",
      "Epoch: 1310 | Loss: 0.46909, Accuracy: 81.60% | Test loss: 0.44919, Test acc: 78.21%\n",
      "Epoch: 1320 | Loss: 0.46906, Accuracy: 81.60% | Test loss: 0.44914, Test acc: 78.21%\n",
      "Epoch: 1330 | Loss: 0.46902, Accuracy: 81.60% | Test loss: 0.44909, Test acc: 78.21%\n",
      "Epoch: 1340 | Loss: 0.46898, Accuracy: 81.60% | Test loss: 0.44904, Test acc: 78.21%\n",
      "Epoch: 1350 | Loss: 0.46894, Accuracy: 81.60% | Test loss: 0.44900, Test acc: 78.21%\n",
      "Epoch: 1360 | Loss: 0.46890, Accuracy: 81.60% | Test loss: 0.44895, Test acc: 78.21%\n",
      "Epoch: 1370 | Loss: 0.46886, Accuracy: 81.60% | Test loss: 0.44890, Test acc: 78.21%\n",
      "Epoch: 1380 | Loss: 0.46882, Accuracy: 81.60% | Test loss: 0.44885, Test acc: 78.21%\n",
      "Epoch: 1390 | Loss: 0.46879, Accuracy: 81.60% | Test loss: 0.44881, Test acc: 78.21%\n",
      "Epoch: 1400 | Loss: 0.46875, Accuracy: 81.60% | Test loss: 0.44876, Test acc: 78.21%\n",
      "Epoch: 1410 | Loss: 0.46871, Accuracy: 81.60% | Test loss: 0.44871, Test acc: 78.21%\n",
      "Epoch: 1420 | Loss: 0.46867, Accuracy: 81.60% | Test loss: 0.44867, Test acc: 78.21%\n",
      "Epoch: 1430 | Loss: 0.46864, Accuracy: 81.60% | Test loss: 0.44862, Test acc: 78.21%\n",
      "Epoch: 1440 | Loss: 0.46860, Accuracy: 81.60% | Test loss: 0.44857, Test acc: 78.21%\n",
      "Epoch: 1450 | Loss: 0.46856, Accuracy: 81.60% | Test loss: 0.44853, Test acc: 78.21%\n",
      "Epoch: 1460 | Loss: 0.46852, Accuracy: 81.60% | Test loss: 0.44848, Test acc: 78.21%\n",
      "Epoch: 1470 | Loss: 0.46849, Accuracy: 81.60% | Test loss: 0.44844, Test acc: 78.21%\n",
      "Epoch: 1480 | Loss: 0.46845, Accuracy: 81.60% | Test loss: 0.44839, Test acc: 78.21%\n",
      "Epoch: 1490 | Loss: 0.46841, Accuracy: 81.60% | Test loss: 0.44834, Test acc: 78.21%\n",
      "Epoch: 1500 | Loss: 0.46838, Accuracy: 81.60% | Test loss: 0.44830, Test acc: 78.21%\n",
      "Epoch: 1510 | Loss: 0.46834, Accuracy: 81.60% | Test loss: 0.44825, Test acc: 78.21%\n",
      "Epoch: 1520 | Loss: 0.46830, Accuracy: 81.60% | Test loss: 0.44821, Test acc: 78.21%\n",
      "Epoch: 1530 | Loss: 0.46827, Accuracy: 81.60% | Test loss: 0.44816, Test acc: 78.21%\n",
      "Epoch: 1540 | Loss: 0.46823, Accuracy: 81.60% | Test loss: 0.44812, Test acc: 78.21%\n",
      "Epoch: 1550 | Loss: 0.46819, Accuracy: 81.60% | Test loss: 0.44807, Test acc: 78.21%\n",
      "Epoch: 1560 | Loss: 0.46816, Accuracy: 81.46% | Test loss: 0.44803, Test acc: 78.21%\n",
      "Epoch: 1570 | Loss: 0.46812, Accuracy: 81.46% | Test loss: 0.44798, Test acc: 78.21%\n",
      "Epoch: 1580 | Loss: 0.46809, Accuracy: 81.46% | Test loss: 0.44794, Test acc: 78.21%\n",
      "Epoch: 1590 | Loss: 0.46805, Accuracy: 81.46% | Test loss: 0.44790, Test acc: 78.21%\n",
      "Epoch: 1600 | Loss: 0.46801, Accuracy: 81.46% | Test loss: 0.44785, Test acc: 78.21%\n",
      "Epoch: 1610 | Loss: 0.46798, Accuracy: 81.46% | Test loss: 0.44781, Test acc: 78.21%\n",
      "Epoch: 1620 | Loss: 0.46794, Accuracy: 81.46% | Test loss: 0.44776, Test acc: 78.21%\n",
      "Epoch: 1630 | Loss: 0.46791, Accuracy: 81.46% | Test loss: 0.44772, Test acc: 78.21%\n",
      "Epoch: 1640 | Loss: 0.46787, Accuracy: 81.46% | Test loss: 0.44768, Test acc: 78.21%\n",
      "Epoch: 1650 | Loss: 0.46784, Accuracy: 81.46% | Test loss: 0.44763, Test acc: 78.21%\n",
      "Epoch: 1660 | Loss: 0.46780, Accuracy: 81.46% | Test loss: 0.44759, Test acc: 78.21%\n",
      "Epoch: 1670 | Loss: 0.46777, Accuracy: 81.46% | Test loss: 0.44754, Test acc: 78.21%\n",
      "Epoch: 1680 | Loss: 0.46773, Accuracy: 81.46% | Test loss: 0.44750, Test acc: 78.21%\n",
      "Epoch: 1690 | Loss: 0.46770, Accuracy: 81.46% | Test loss: 0.44746, Test acc: 78.21%\n",
      "Epoch: 1700 | Loss: 0.46766, Accuracy: 81.46% | Test loss: 0.44741, Test acc: 78.21%\n",
      "Epoch: 1710 | Loss: 0.46763, Accuracy: 81.46% | Test loss: 0.44737, Test acc: 78.21%\n",
      "Epoch: 1720 | Loss: 0.46760, Accuracy: 81.46% | Test loss: 0.44733, Test acc: 78.21%\n",
      "Epoch: 1730 | Loss: 0.46756, Accuracy: 81.46% | Test loss: 0.44729, Test acc: 78.21%\n",
      "Epoch: 1740 | Loss: 0.46753, Accuracy: 81.46% | Test loss: 0.44724, Test acc: 78.21%\n",
      "Epoch: 1750 | Loss: 0.46749, Accuracy: 81.46% | Test loss: 0.44720, Test acc: 78.21%\n",
      "Epoch: 1760 | Loss: 0.46746, Accuracy: 81.46% | Test loss: 0.44716, Test acc: 78.21%\n",
      "Epoch: 1770 | Loss: 0.46743, Accuracy: 81.46% | Test loss: 0.44712, Test acc: 78.21%\n",
      "Epoch: 1780 | Loss: 0.46739, Accuracy: 81.46% | Test loss: 0.44707, Test acc: 78.21%\n",
      "Epoch: 1790 | Loss: 0.46736, Accuracy: 81.46% | Test loss: 0.44703, Test acc: 78.21%\n",
      "Epoch: 1800 | Loss: 0.46732, Accuracy: 81.46% | Test loss: 0.44699, Test acc: 78.21%\n",
      "Epoch: 1810 | Loss: 0.46729, Accuracy: 81.46% | Test loss: 0.44695, Test acc: 78.21%\n",
      "Epoch: 1820 | Loss: 0.46726, Accuracy: 81.46% | Test loss: 0.44691, Test acc: 78.21%\n",
      "Epoch: 1830 | Loss: 0.46723, Accuracy: 81.46% | Test loss: 0.44687, Test acc: 78.21%\n",
      "Epoch: 1840 | Loss: 0.46719, Accuracy: 81.46% | Test loss: 0.44682, Test acc: 78.21%\n",
      "Epoch: 1850 | Loss: 0.46716, Accuracy: 81.46% | Test loss: 0.44678, Test acc: 78.21%\n",
      "Epoch: 1860 | Loss: 0.46713, Accuracy: 81.46% | Test loss: 0.44674, Test acc: 78.21%\n",
      "Epoch: 1870 | Loss: 0.46709, Accuracy: 81.46% | Test loss: 0.44670, Test acc: 78.21%\n",
      "Epoch: 1880 | Loss: 0.46706, Accuracy: 81.46% | Test loss: 0.44666, Test acc: 78.21%\n",
      "Epoch: 1890 | Loss: 0.46703, Accuracy: 81.46% | Test loss: 0.44662, Test acc: 78.21%\n",
      "Epoch: 1900 | Loss: 0.46700, Accuracy: 81.46% | Test loss: 0.44658, Test acc: 78.21%\n",
      "Epoch: 1910 | Loss: 0.46696, Accuracy: 81.32% | Test loss: 0.44654, Test acc: 78.21%\n",
      "Epoch: 1920 | Loss: 0.46693, Accuracy: 81.32% | Test loss: 0.44650, Test acc: 78.21%\n",
      "Epoch: 1930 | Loss: 0.46690, Accuracy: 81.32% | Test loss: 0.44645, Test acc: 78.21%\n",
      "Epoch: 1940 | Loss: 0.46687, Accuracy: 81.32% | Test loss: 0.44641, Test acc: 78.21%\n",
      "Epoch: 1950 | Loss: 0.46684, Accuracy: 81.32% | Test loss: 0.44637, Test acc: 78.21%\n",
      "Epoch: 1960 | Loss: 0.46680, Accuracy: 81.32% | Test loss: 0.44633, Test acc: 78.21%\n",
      "Epoch: 1970 | Loss: 0.46677, Accuracy: 81.32% | Test loss: 0.44629, Test acc: 78.21%\n",
      "Epoch: 1980 | Loss: 0.46674, Accuracy: 81.32% | Test loss: 0.44625, Test acc: 78.21%\n",
      "Epoch: 1990 | Loss: 0.46671, Accuracy: 81.32% | Test loss: 0.44621, Test acc: 78.21%\n",
      "Epoch: 2000 | Loss: 0.46668, Accuracy: 81.32% | Test loss: 0.44617, Test acc: 78.21%\n",
      "Epoch: 2010 | Loss: 0.46665, Accuracy: 81.32% | Test loss: 0.44613, Test acc: 78.21%\n",
      "Epoch: 2020 | Loss: 0.46661, Accuracy: 81.32% | Test loss: 0.44609, Test acc: 78.21%\n",
      "Epoch: 2030 | Loss: 0.46658, Accuracy: 81.32% | Test loss: 0.44605, Test acc: 78.21%\n",
      "Epoch: 2040 | Loss: 0.46655, Accuracy: 81.32% | Test loss: 0.44602, Test acc: 78.21%\n",
      "Epoch: 2050 | Loss: 0.46652, Accuracy: 81.32% | Test loss: 0.44598, Test acc: 78.21%\n",
      "Epoch: 2060 | Loss: 0.46649, Accuracy: 81.32% | Test loss: 0.44594, Test acc: 78.21%\n",
      "Epoch: 2070 | Loss: 0.46646, Accuracy: 81.32% | Test loss: 0.44590, Test acc: 78.21%\n",
      "Epoch: 2080 | Loss: 0.46643, Accuracy: 81.32% | Test loss: 0.44586, Test acc: 78.21%\n",
      "Epoch: 2090 | Loss: 0.46640, Accuracy: 81.32% | Test loss: 0.44582, Test acc: 78.21%\n",
      "Epoch: 2100 | Loss: 0.46637, Accuracy: 81.32% | Test loss: 0.44578, Test acc: 78.21%\n",
      "Epoch: 2110 | Loss: 0.46634, Accuracy: 81.32% | Test loss: 0.44574, Test acc: 78.21%\n",
      "Epoch: 2120 | Loss: 0.46631, Accuracy: 81.32% | Test loss: 0.44570, Test acc: 78.21%\n",
      "Epoch: 2130 | Loss: 0.46628, Accuracy: 81.32% | Test loss: 0.44567, Test acc: 78.21%\n",
      "Epoch: 2140 | Loss: 0.46625, Accuracy: 81.32% | Test loss: 0.44563, Test acc: 78.21%\n",
      "Epoch: 2150 | Loss: 0.46622, Accuracy: 81.32% | Test loss: 0.44559, Test acc: 78.21%\n",
      "Epoch: 2160 | Loss: 0.46618, Accuracy: 81.32% | Test loss: 0.44555, Test acc: 78.21%\n",
      "Epoch: 2170 | Loss: 0.46616, Accuracy: 81.32% | Test loss: 0.44551, Test acc: 78.21%\n",
      "Epoch: 2180 | Loss: 0.46613, Accuracy: 81.32% | Test loss: 0.44547, Test acc: 78.21%\n",
      "Epoch: 2190 | Loss: 0.46610, Accuracy: 81.32% | Test loss: 0.44544, Test acc: 78.21%\n",
      "Epoch: 2200 | Loss: 0.46607, Accuracy: 81.32% | Test loss: 0.44540, Test acc: 78.21%\n",
      "Epoch: 2210 | Loss: 0.46604, Accuracy: 81.32% | Test loss: 0.44536, Test acc: 78.21%\n",
      "Epoch: 2220 | Loss: 0.46601, Accuracy: 81.32% | Test loss: 0.44532, Test acc: 78.21%\n",
      "Epoch: 2230 | Loss: 0.46598, Accuracy: 81.32% | Test loss: 0.44528, Test acc: 78.21%\n",
      "Epoch: 2240 | Loss: 0.46595, Accuracy: 81.32% | Test loss: 0.44525, Test acc: 78.21%\n",
      "Epoch: 2250 | Loss: 0.46592, Accuracy: 81.32% | Test loss: 0.44521, Test acc: 78.21%\n",
      "Epoch: 2260 | Loss: 0.46589, Accuracy: 81.32% | Test loss: 0.44517, Test acc: 78.21%\n",
      "Epoch: 2270 | Loss: 0.46586, Accuracy: 81.32% | Test loss: 0.44514, Test acc: 78.21%\n",
      "Epoch: 2280 | Loss: 0.46583, Accuracy: 81.32% | Test loss: 0.44510, Test acc: 78.21%\n",
      "Epoch: 2290 | Loss: 0.46580, Accuracy: 81.18% | Test loss: 0.44506, Test acc: 78.21%\n",
      "Epoch: 2300 | Loss: 0.46577, Accuracy: 81.18% | Test loss: 0.44502, Test acc: 78.21%\n",
      "Epoch: 2310 | Loss: 0.46574, Accuracy: 81.18% | Test loss: 0.44499, Test acc: 78.21%\n",
      "Epoch: 2320 | Loss: 0.46572, Accuracy: 81.18% | Test loss: 0.44495, Test acc: 78.21%\n",
      "Epoch: 2330 | Loss: 0.46569, Accuracy: 81.18% | Test loss: 0.44491, Test acc: 78.21%\n",
      "Epoch: 2340 | Loss: 0.46566, Accuracy: 81.18% | Test loss: 0.44488, Test acc: 78.21%\n",
      "Epoch: 2350 | Loss: 0.46563, Accuracy: 81.18% | Test loss: 0.44484, Test acc: 78.21%\n",
      "Epoch: 2360 | Loss: 0.46560, Accuracy: 81.18% | Test loss: 0.44480, Test acc: 78.21%\n",
      "Epoch: 2370 | Loss: 0.46557, Accuracy: 81.18% | Test loss: 0.44477, Test acc: 78.21%\n",
      "Epoch: 2380 | Loss: 0.46555, Accuracy: 81.18% | Test loss: 0.44473, Test acc: 78.21%\n",
      "Epoch: 2390 | Loss: 0.46552, Accuracy: 81.18% | Test loss: 0.44470, Test acc: 78.21%\n",
      "Epoch: 2400 | Loss: 0.46549, Accuracy: 81.18% | Test loss: 0.44466, Test acc: 78.21%\n",
      "Epoch: 2410 | Loss: 0.46546, Accuracy: 81.18% | Test loss: 0.44462, Test acc: 78.21%\n",
      "Epoch: 2420 | Loss: 0.46543, Accuracy: 81.18% | Test loss: 0.44459, Test acc: 78.21%\n",
      "Epoch: 2430 | Loss: 0.46541, Accuracy: 81.18% | Test loss: 0.44455, Test acc: 78.21%\n",
      "Epoch: 2440 | Loss: 0.46538, Accuracy: 81.18% | Test loss: 0.44452, Test acc: 78.21%\n",
      "Epoch: 2450 | Loss: 0.46535, Accuracy: 81.18% | Test loss: 0.44448, Test acc: 78.21%\n",
      "Epoch: 2460 | Loss: 0.46532, Accuracy: 81.18% | Test loss: 0.44444, Test acc: 78.21%\n",
      "Epoch: 2470 | Loss: 0.46530, Accuracy: 81.18% | Test loss: 0.44441, Test acc: 78.21%\n",
      "Epoch: 2480 | Loss: 0.46527, Accuracy: 81.18% | Test loss: 0.44437, Test acc: 78.21%\n",
      "Epoch: 2490 | Loss: 0.46524, Accuracy: 81.18% | Test loss: 0.44434, Test acc: 78.21%\n",
      "Epoch: 2500 | Loss: 0.46521, Accuracy: 81.18% | Test loss: 0.44430, Test acc: 78.21%\n",
      "Epoch: 2510 | Loss: 0.46519, Accuracy: 81.18% | Test loss: 0.44427, Test acc: 78.21%\n",
      "Epoch: 2520 | Loss: 0.46516, Accuracy: 81.18% | Test loss: 0.44423, Test acc: 78.21%\n",
      "Epoch: 2530 | Loss: 0.46513, Accuracy: 81.18% | Test loss: 0.44420, Test acc: 78.21%\n",
      "Epoch: 2540 | Loss: 0.46510, Accuracy: 81.18% | Test loss: 0.44416, Test acc: 78.21%\n",
      "Epoch: 2550 | Loss: 0.46508, Accuracy: 81.18% | Test loss: 0.44413, Test acc: 78.21%\n",
      "Epoch: 2560 | Loss: 0.46505, Accuracy: 81.18% | Test loss: 0.44409, Test acc: 78.21%\n",
      "Epoch: 2570 | Loss: 0.46502, Accuracy: 81.18% | Test loss: 0.44406, Test acc: 78.21%\n",
      "Epoch: 2580 | Loss: 0.46500, Accuracy: 81.18% | Test loss: 0.44402, Test acc: 78.21%\n",
      "Epoch: 2590 | Loss: 0.46497, Accuracy: 81.18% | Test loss: 0.44399, Test acc: 78.21%\n",
      "Epoch: 2600 | Loss: 0.46494, Accuracy: 81.18% | Test loss: 0.44396, Test acc: 78.21%\n",
      "Epoch: 2610 | Loss: 0.46492, Accuracy: 81.18% | Test loss: 0.44392, Test acc: 78.21%\n",
      "Epoch: 2620 | Loss: 0.46489, Accuracy: 81.18% | Test loss: 0.44389, Test acc: 78.21%\n",
      "Epoch: 2630 | Loss: 0.46487, Accuracy: 81.18% | Test loss: 0.44385, Test acc: 78.21%\n",
      "Epoch: 2640 | Loss: 0.46484, Accuracy: 81.18% | Test loss: 0.44382, Test acc: 78.21%\n",
      "Epoch: 2650 | Loss: 0.46481, Accuracy: 81.18% | Test loss: 0.44378, Test acc: 78.21%\n",
      "Epoch: 2660 | Loss: 0.46479, Accuracy: 81.18% | Test loss: 0.44375, Test acc: 78.21%\n",
      "Epoch: 2670 | Loss: 0.46476, Accuracy: 81.18% | Test loss: 0.44372, Test acc: 78.21%\n",
      "Epoch: 2680 | Loss: 0.46473, Accuracy: 81.18% | Test loss: 0.44368, Test acc: 78.21%\n",
      "Epoch: 2690 | Loss: 0.46471, Accuracy: 81.18% | Test loss: 0.44365, Test acc: 78.21%\n",
      "Epoch: 2700 | Loss: 0.46468, Accuracy: 81.18% | Test loss: 0.44361, Test acc: 78.21%\n",
      "Epoch: 2710 | Loss: 0.46466, Accuracy: 81.18% | Test loss: 0.44358, Test acc: 78.21%\n",
      "Epoch: 2720 | Loss: 0.46463, Accuracy: 81.18% | Test loss: 0.44355, Test acc: 78.77%\n",
      "Epoch: 2730 | Loss: 0.46461, Accuracy: 81.18% | Test loss: 0.44351, Test acc: 78.77%\n",
      "Epoch: 2740 | Loss: 0.46458, Accuracy: 81.18% | Test loss: 0.44348, Test acc: 78.77%\n",
      "Epoch: 2750 | Loss: 0.46455, Accuracy: 81.18% | Test loss: 0.44345, Test acc: 78.77%\n",
      "Epoch: 2760 | Loss: 0.46453, Accuracy: 81.18% | Test loss: 0.44341, Test acc: 78.77%\n",
      "Epoch: 2770 | Loss: 0.46450, Accuracy: 81.18% | Test loss: 0.44338, Test acc: 78.77%\n",
      "Epoch: 2780 | Loss: 0.46448, Accuracy: 81.18% | Test loss: 0.44335, Test acc: 78.77%\n",
      "Epoch: 2790 | Loss: 0.46445, Accuracy: 81.18% | Test loss: 0.44332, Test acc: 78.77%\n",
      "Epoch: 2800 | Loss: 0.46443, Accuracy: 81.18% | Test loss: 0.44328, Test acc: 78.77%\n",
      "Epoch: 2810 | Loss: 0.46440, Accuracy: 81.18% | Test loss: 0.44325, Test acc: 78.77%\n",
      "Epoch: 2820 | Loss: 0.46438, Accuracy: 81.18% | Test loss: 0.44322, Test acc: 78.77%\n",
      "Epoch: 2830 | Loss: 0.46435, Accuracy: 81.18% | Test loss: 0.44318, Test acc: 78.77%\n",
      "Epoch: 2840 | Loss: 0.46433, Accuracy: 81.18% | Test loss: 0.44315, Test acc: 78.77%\n",
      "Epoch: 2850 | Loss: 0.46430, Accuracy: 81.18% | Test loss: 0.44312, Test acc: 78.77%\n",
      "Epoch: 2860 | Loss: 0.46428, Accuracy: 81.18% | Test loss: 0.44309, Test acc: 78.77%\n",
      "Epoch: 2870 | Loss: 0.46425, Accuracy: 81.18% | Test loss: 0.44305, Test acc: 78.77%\n",
      "Epoch: 2880 | Loss: 0.46423, Accuracy: 81.18% | Test loss: 0.44302, Test acc: 78.77%\n",
      "Epoch: 2890 | Loss: 0.46421, Accuracy: 81.18% | Test loss: 0.44299, Test acc: 78.77%\n",
      "Epoch: 2900 | Loss: 0.46418, Accuracy: 81.18% | Test loss: 0.44296, Test acc: 78.77%\n",
      "Epoch: 2910 | Loss: 0.46416, Accuracy: 81.18% | Test loss: 0.44292, Test acc: 78.77%\n",
      "Epoch: 2920 | Loss: 0.46413, Accuracy: 81.32% | Test loss: 0.44289, Test acc: 78.77%\n",
      "Epoch: 2930 | Loss: 0.46411, Accuracy: 81.32% | Test loss: 0.44286, Test acc: 78.77%\n",
      "Epoch: 2940 | Loss: 0.46408, Accuracy: 81.32% | Test loss: 0.44283, Test acc: 78.77%\n",
      "Epoch: 2950 | Loss: 0.46406, Accuracy: 81.32% | Test loss: 0.44280, Test acc: 78.77%\n",
      "Epoch: 2960 | Loss: 0.46404, Accuracy: 81.32% | Test loss: 0.44277, Test acc: 78.77%\n",
      "Epoch: 2970 | Loss: 0.46401, Accuracy: 81.32% | Test loss: 0.44273, Test acc: 78.77%\n",
      "Epoch: 2980 | Loss: 0.46399, Accuracy: 81.32% | Test loss: 0.44270, Test acc: 78.77%\n",
      "Epoch: 2990 | Loss: 0.46396, Accuracy: 81.32% | Test loss: 0.44267, Test acc: 78.77%\n",
      "Epoch: 3000 | Loss: 0.46394, Accuracy: 81.32% | Test loss: 0.44264, Test acc: 78.77%\n",
      "Epoch: 3010 | Loss: 0.46392, Accuracy: 81.32% | Test loss: 0.44261, Test acc: 78.21%\n",
      "Epoch: 3020 | Loss: 0.46389, Accuracy: 81.32% | Test loss: 0.44258, Test acc: 78.21%\n",
      "Epoch: 3030 | Loss: 0.46387, Accuracy: 81.32% | Test loss: 0.44254, Test acc: 78.21%\n",
      "Epoch: 3040 | Loss: 0.46384, Accuracy: 81.32% | Test loss: 0.44251, Test acc: 78.21%\n",
      "Epoch: 3050 | Loss: 0.46382, Accuracy: 81.18% | Test loss: 0.44248, Test acc: 78.21%\n",
      "Epoch: 3060 | Loss: 0.46380, Accuracy: 81.18% | Test loss: 0.44245, Test acc: 78.21%\n",
      "Epoch: 3070 | Loss: 0.46377, Accuracy: 81.18% | Test loss: 0.44242, Test acc: 78.21%\n",
      "Epoch: 3080 | Loss: 0.46375, Accuracy: 81.18% | Test loss: 0.44239, Test acc: 78.21%\n",
      "Epoch: 3090 | Loss: 0.46373, Accuracy: 81.18% | Test loss: 0.44236, Test acc: 78.21%\n",
      "Epoch: 3100 | Loss: 0.46370, Accuracy: 81.18% | Test loss: 0.44233, Test acc: 78.21%\n",
      "Epoch: 3110 | Loss: 0.46368, Accuracy: 81.18% | Test loss: 0.44230, Test acc: 78.21%\n",
      "Epoch: 3120 | Loss: 0.46366, Accuracy: 81.18% | Test loss: 0.44227, Test acc: 78.21%\n",
      "Epoch: 3130 | Loss: 0.46363, Accuracy: 81.18% | Test loss: 0.44223, Test acc: 78.21%\n",
      "Epoch: 3140 | Loss: 0.46361, Accuracy: 81.18% | Test loss: 0.44220, Test acc: 78.21%\n",
      "Epoch: 3150 | Loss: 0.46359, Accuracy: 81.18% | Test loss: 0.44217, Test acc: 78.21%\n",
      "Epoch: 3160 | Loss: 0.46357, Accuracy: 81.18% | Test loss: 0.44214, Test acc: 78.21%\n",
      "Epoch: 3170 | Loss: 0.46354, Accuracy: 81.18% | Test loss: 0.44211, Test acc: 78.21%\n",
      "Epoch: 3180 | Loss: 0.46352, Accuracy: 81.18% | Test loss: 0.44208, Test acc: 78.21%\n",
      "Epoch: 3190 | Loss: 0.46350, Accuracy: 81.18% | Test loss: 0.44205, Test acc: 78.21%\n",
      "Epoch: 3200 | Loss: 0.46348, Accuracy: 81.18% | Test loss: 0.44202, Test acc: 78.21%\n",
      "Epoch: 3210 | Loss: 0.46345, Accuracy: 81.18% | Test loss: 0.44199, Test acc: 78.21%\n",
      "Epoch: 3220 | Loss: 0.46343, Accuracy: 81.18% | Test loss: 0.44196, Test acc: 78.21%\n",
      "Epoch: 3230 | Loss: 0.46341, Accuracy: 81.18% | Test loss: 0.44193, Test acc: 78.21%\n",
      "Epoch: 3240 | Loss: 0.46339, Accuracy: 81.18% | Test loss: 0.44190, Test acc: 78.21%\n",
      "Epoch: 3250 | Loss: 0.46336, Accuracy: 81.18% | Test loss: 0.44187, Test acc: 78.21%\n",
      "Epoch: 3260 | Loss: 0.46334, Accuracy: 81.18% | Test loss: 0.44184, Test acc: 78.21%\n",
      "Epoch: 3270 | Loss: 0.46332, Accuracy: 81.18% | Test loss: 0.44181, Test acc: 78.21%\n",
      "Epoch: 3280 | Loss: 0.46330, Accuracy: 81.18% | Test loss: 0.44178, Test acc: 78.21%\n",
      "Epoch: 3290 | Loss: 0.46327, Accuracy: 81.18% | Test loss: 0.44175, Test acc: 78.21%\n",
      "Epoch: 3300 | Loss: 0.46325, Accuracy: 81.18% | Test loss: 0.44172, Test acc: 78.21%\n",
      "Epoch: 3310 | Loss: 0.46323, Accuracy: 81.18% | Test loss: 0.44169, Test acc: 78.21%\n",
      "Epoch: 3320 | Loss: 0.46321, Accuracy: 81.18% | Test loss: 0.44166, Test acc: 78.21%\n",
      "Epoch: 3330 | Loss: 0.46319, Accuracy: 81.18% | Test loss: 0.44163, Test acc: 78.21%\n",
      "Epoch: 3340 | Loss: 0.46316, Accuracy: 81.18% | Test loss: 0.44161, Test acc: 78.21%\n",
      "Epoch: 3350 | Loss: 0.46314, Accuracy: 81.18% | Test loss: 0.44158, Test acc: 78.21%\n",
      "Epoch: 3360 | Loss: 0.46312, Accuracy: 81.18% | Test loss: 0.44155, Test acc: 78.21%\n",
      "Epoch: 3370 | Loss: 0.46310, Accuracy: 81.18% | Test loss: 0.44152, Test acc: 78.21%\n",
      "Epoch: 3380 | Loss: 0.46308, Accuracy: 81.18% | Test loss: 0.44149, Test acc: 78.21%\n",
      "Epoch: 3390 | Loss: 0.46306, Accuracy: 81.18% | Test loss: 0.44146, Test acc: 78.21%\n",
      "Epoch: 3400 | Loss: 0.46303, Accuracy: 81.18% | Test loss: 0.44143, Test acc: 78.21%\n",
      "Epoch: 3410 | Loss: 0.46301, Accuracy: 81.18% | Test loss: 0.44140, Test acc: 78.21%\n",
      "Epoch: 3420 | Loss: 0.46299, Accuracy: 81.18% | Test loss: 0.44137, Test acc: 78.21%\n",
      "Epoch: 3430 | Loss: 0.46297, Accuracy: 81.18% | Test loss: 0.44134, Test acc: 78.21%\n",
      "Epoch: 3440 | Loss: 0.46295, Accuracy: 81.18% | Test loss: 0.44131, Test acc: 78.21%\n",
      "Epoch: 3450 | Loss: 0.46293, Accuracy: 81.18% | Test loss: 0.44129, Test acc: 78.21%\n",
      "Epoch: 3460 | Loss: 0.46291, Accuracy: 81.18% | Test loss: 0.44126, Test acc: 78.21%\n",
      "Epoch: 3470 | Loss: 0.46288, Accuracy: 81.18% | Test loss: 0.44123, Test acc: 78.21%\n",
      "Epoch: 3480 | Loss: 0.46286, Accuracy: 81.18% | Test loss: 0.44120, Test acc: 78.21%\n",
      "Epoch: 3490 | Loss: 0.46284, Accuracy: 81.18% | Test loss: 0.44117, Test acc: 78.21%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88) # semilla aleatoria\n",
    "\n",
    "# N√∫mero de epocas\n",
    "epochs = 3500\n",
    "\n",
    "# Poner los datos en el hardware target\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop de training y eval\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward propagation (el modelo regresa logits)\n",
    "    y_logits = model_1(X_train).squeeze() # squeeze para remover `1` dimension extra\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Se calcula loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss acepta los logits de salida\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Zero grad para el optimizador\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizador\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Evaluacion\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward \n",
    "        test_logits = model_1(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print cada 10 epocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
